> 도서 : 밑바닥 부터 시작하는 딥러닝 [링크](https://www.hanbit.co.kr/store/books/look.php?p_code=B8475831198) [깃허브](https://github.com/WegraLee/deep-learning-from-scratch)

# 5장 오차역전파법

`오차역전파법`은 가중치 매개변수의 기울기를 효율적으로 계산하는 방법이다.

## 5.1 계산 그래프

`계산그래프`는 계산 과정을 그래프로 나타낸 것이다. 여기서 `그래프`는 자료구조의 그래프로 복수의`노드`와 `에지`로 표현된다.

### 5.1.1. 계산 그래프로 풀다

계산 그래프가 어떤 것인지 알기 위해 간단한 문제를 풀어보자

> 문제 1 : 현빈 군은 슈퍼에서 1개에 100원인 사과를 2개 샀습니다. 이때 지불 금액을 구하세요. 단, 소비세가 10% 부과됩니다.

> 문제1의 계산그래프

<img src="5장_오차역전파법.assets/fig 5-1.png">

> 사과의 개수와 소비세를 변수 취급한 계산그래프

<img src="5장_오차역전파법.assets/fig 5-2.png">

> 문제 2 : 현빈 군은 슈퍼에서 사과를 2개, 귤을 3개 샀습니다. 사과는 1개에 100원, 귤을 1개 150원입니다. 소비세가 10%일 때 지불 금액을 구하세요.

> 문제2의 계산그래프

<img src="5장_오차역전파법.assets/fig 5-3.png">

`계산그래프`를 이용한 문제풀이의 흐름

1. 계산그래프를 구성한다.
2. 그래프에서 계산을 왼쪽에서 오른쪽으로 진행한다.

여기서 `계산을 왼쪽에서 오른쪽으로 진행`하는 단계를 `순전파`라고 한다. 그리고 반대 방향의 흐름을 `역전파`라고 한다.

### 5.1.2 국소적 계산

계산그래프의 특징은 `국소적 계산`을 전파함으로써 최종 결과를 얻는다는 점이다. 국소적이란 `자신과 직접 관계된 작은 범위`라는 뜻이다.

예를들어 사과 2개를 포함한 여러 식품을 구입하는 경우를 보자.

<img src="5장_오차역전파법.assets/fig 5-4.png">

여기서 핵심은 각 `노드`에서의 계산은 국소적 계산이라는 점이다. 사과와 그 외의 물풀 값을 더하는 계산에서 4000은 어떻게 계산되었는지는 상관없이 단지 두 숫자`4000,200`을 더하면 된다는 뜻이다.

이처럼 계산그래프는 `국소적 계산`에 집중함으로써 전체의 복잡한 계산을 해낼 수 있다.

### 5.1.3 왜 계산 그래프로 푸는가?

> 계산그래프의 이점

1. `국소적 계산` : 아무리 복잡해도 각 노드에서는 단순한 계산에 집중해서 문제를 단순화 할 수 있다.
2. 중간 계산 결과를 모두 보관할 수 있다.
3. 역전파를 통해 `미분`를 효율적으로 계산할 수 있다.

가령 문제1에서 `사과 가격이 오르면 최종 금액에 어떤 영향`을 끼치는지를 알고 싶다는 것은 `사과 가격에 대한 지불 금액의 미분`을 구하는 것과 동일하다.

<img src="5장_오차역전파법.assets/fig 5-5.png">

## 5.2 연쇄법칙

`역전파`는 `국소적인 미분`을 순방향과는 반대로 전달한다. 또한, `국소적 미분`을 전달하는 원리는 `연쇄법칙`에 따른다.

### 5.2.1 계산 그래프의 역전파

> 역전파의 예시 그림

<img src="5장_오차역전파법.assets/fig 5-6.png">

위 그림에서 역전파의 계산 절차는 신호 E에 노드의 국소적미분을 곱한 후 다음 노드로 전달하는 것이다.

여기서 국소적 미분은 x`입력`에 대한 y`출력`의 미분을 뜻한다. 그리고 이 국소적 미분을 사류에서 전달된 값`E`에 곱해 앞쪽 노드로 전달한다.

`입력의 변화에 따라 출력이 어떻게 변화하나? -> 기울기(미분)`

### 5.2.2 연쇄법칙?

연쇄법칙을 설명하기 위해서는 합성함수의 이해가 필요하다. `합성함수`란 여러 함수로 구성된 함수이다.

예를 들어 z = (x+y)^2는 

<img src="5장_오차역전파법.assets/e 5.1.png">

로 표현가능하다.

연쇄법칙은 합성 함수의 미분에 대한 성질이며

> 합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다.

로 정의 된다.

> 예를들면 위의 식에서 `X에 대한 z의 미분`은 `t에 대한 z의 미분`과 `x에 대한 t의 미분`의 곱으로 나타낼 수 있다는 의미이다. 

<img src="5장_오차역전파법.assets/e 5.4.png">



### 5.2.3 연쇄법칙과 계산 그래프

> 위의 식의 계산 그래프 : 순전파와 반대 방향으로 편미분을 곱해서 전달한다.

<img src="5장_오차역전파법.assets/fig 5-7.png">

역전파에서는 노드로 들어온 `입력 신호`에 그 노드의 `편미분`을 곱해서 다음 노드로 전달한다.

이 계산 그래프는 연쇄법칙에 따르면 

<img src="5장_오차역전파법.assets/e 5.4.png">

가 성립되어 `x에 대한 z의 미분`이 된다. 즉, 연쇄법칙의 원리가 `역전파`에 적용된다는 뜻이다.

## 5.3 역전파

### 5.3.1 덧셈 노드의 역전파

> z = x + y 의 미분의 해석과 계산 그래프

<img src="5장_오차역전파법.assets/e 5.5.png">

<img src="5장_오차역전파법.assets/fig 5-9.png">

그림과 같이 역전파 때는 상류에서 전해진 미분에 1을 곱하여 하류로 흘린다. 

즉, `덧셈 노드`의 역전파는 1을 곱하기만 할 뿐이고, `입력된 값을 그대로` 다음 노드로 보낸다.

### 5.3.2 곱셈 노드의 역전파

> z = xy 의 미분의 해석과 계산 그래프

<img src="5장_오차역전파법.assets/e 5.6.png">

<img src="5장_오차역전파법.assets/fig 5-12.png">

`곱셈 노드`역전파는 상류의 값에 순전파 때의 입력신호들을 `서로 바꾼 값`을 곱해서 하류로 보낸다.

### 5.3.3 사과 쇼핑의 예

<img src="5장_오차역전파법.assets/fig 5-14.png">

`곱셈 노드`에서 설명한 것 처럼 입력 신호를 서로 교차해서 곱한 뒤 하류로 전달하는 것을 볼 수 있다.

그림에서 보면 사과 가격의 미분은 `2.2`, 사과 개수의 미분은 `110` 소비세의 미분은 `200`이다.

이는 사과 가격과 소비세가 같은 양만큼 증가하면 사과 가격은 `2.2`, 소비세는 `200`만큼의 영향을 준다는 것으로 해석할 수 있다.



## 5.4 단순한 계층 구현하기

### 5.4.1 곱셈계층

> 곱셈계층 구현과 계산 그래프 구현 코드

<img src="5장_오차역전파법.assets/fig 5-16.png">

```python
class MulLayer:
    def __init__(self):
        # x, y 초기화
        self.x = None
        self.y = None

    # 순전파
    def forward(self, x, y):
        # 입력 값 저장
        self.x = x
        self.y = y
        out = x * y
        return out

    # 역전파
    def backward(self, dout):
        # 출력의 미분(dout)에 x와 y를 서로 교차해서 곱하기
        dx = dout * self.y
        dy = dout * self.x
        return dx, dy


if __name__ == "__main__":
    apple = 100
    apple_num = 2
    tax = 1.1

    # 계층 생성
    mul_apple_layer = MulLayer()
    mul_tax_layer = MulLayer()

    # 순전판
    apple_price = mul_apple_layer.forward(apple, apple_num)
    price = mul_tax_layer.forward(apple_price, tax)
    print(price)

    # 역전파
    dprice = 1
    # 순전파의 반대 반향
    dapple_price, dtax = mul_tax_layer.backward(dprice)
    dapple, dapple_num = mul_apple_layer.backward(dapple_price)
    print(dapple, dapple_num, dtax)

```

### 5.4.2 덧셈계층

```python
class AddLayer:
    def __init__(self):
        pass

    # 순전파
    def forward(self, x, y):
        out = x + y
        return out

    # 역전파
    def backward(self, dout):
        # 출력의 미분에 1을 곱하기
        dx = dout * 1
        dy = dout * 1
        return dx, dy
```

<img src="5장_오차역전파법.assets/fig 5-17.png">

> 위 계산 그래프 코드 순전파 역전파 구현

```python
# 덧셈 계층
class AddLayer:
    def __init__(self):
        pass

    # 순전파
    def forward(self, x, y):
        out = x + y
        return out

    # 역전파
    def backward(self, dout):
        # 출력의 미분에 1을 곱하기
        dx = dout * 1
        dy = dout * 1
        return dx, dy

# 곱셈 계층
class MulLayer:
    def __init__(self):
        self.x = None
        self.y = None

    def forward(self, x, y):
        self.x = x
        self.y = y
        out = x * y
        return out

    def backward(self, dout):
        dx = dout * self.y
        dy = dout * self.x
        return dx, dy


if __name__ == "__main__":
    apple = 100
    apple_num = 2
    orange = 150
    orange_num = 3
    tax = 1.1

    # 계층 생성
    mul_apple_layer = MulLayer()
    mul_orange_layer = MulLayer()
    add_apple_orange_layer = AddLayer()
    mul_price_layer = MulLayer()

    # 순전판
    # 사과 가격 계산
    apple_price = mul_apple_layer.forward(apple, apple_num)
    # 귤 가격 계산
    orange_price = mul_orange_layer.forward(orange, orange_num)
    # 사과 + 귤 계산
    add_price = add_apple_orange_layer.forward(apple_price, orange_price)
    mul_price = mul_price_layer.forward(add_price, tax)

    print(mul_price) # 715.0000000000001

    # 역전파
    # 출력의 미분 값
    dprice = 1
    # 순전파의 반대 순서
    add_dprice, dtax = mul_price_layer.backward(dprice)
    apple_dprice, orange_dprice = add_apple_orange_layer.backward(add_dprice)
    dorange, orange_dnum = mul_orange_layer.backward(orange_dprice)
    dapple, apple_dnum = mul_apple_layer.backward(apple_dprice)

    print(add_dprice, dtax, apple_dprice, orange_dprice) # 1.1 650 1.1 1.1
    print(dorange, orange_dnum) # 3.3000000000000003 165.0
    print(dapple, apple_dnum) # 2.2 110.00000000000001
```

## 5.5 활성화 함수 계층 구현하기

### 5.5.1 ReLU 계층

> ReLU 함수 수식과 x에 대한 y의 미분

<img src="5장_오차역전파법.assets/e 5.7.png">

<img src="5장_오차역전파법.assets/e 5.8.png">

입력`x` 이 `0` 보다 크면 출력을 그대로  전달하고, 입력`x`가 `0` 이하 라면 0을  전달한다. 

> ReLU 계산그래프

<img src="5장_오차역전파법.assets/fig 5-18.png">

```python
class ReLU:
    def __init__(self):
        self.mask = None

    def forward(self, x):
        # 입력 행렬에서 0 이하인 인덱스 True
        # EX) [[1 -0.5][-2 3]] => [[False True] [True False]]
        self.mask = x <= 0
        out = x.copy()
        # 입력 행렬에서 0 이하인 인덱스 값 0으로 초기화
        # EX) [[1 -0.5][-2 3]] => [[1 0] [0 3]]
        out[self.mask] = 0
        return out

    def backward(self, dout):
        # 순전파 때 만들어둔 mask를 사용해 상류에서 전파된 dout에서 0 이하 값 인덱스 0으로 초기화
        dout[self.mask] = 0
        dx = dout
        return dx

```

### 5.5.2 Sigmoid 계층

> 시그모이드 순전파와 역전파

<img src="5장_오차역전파법.assets/fig 5-19.png">

<img src="5장_오차역전파법.assets/fig 5-20.png">

`/`와 `exp` 연산이 처음 등장하였다.

일단, `exp`연산은 순전파와 역전파에서 동일하게 `exp(x)`을 전달한다.

`/`연산은 순전파에서는` 1/x` 를 역전파에서는 상류에서 온 값에 `-y^2 `를 곱해서 전달한다.

> 시그모이드 계산 그래프 간소화 버전과 y로 정리한 버전 그리고 구현 코드

<img src="5장_오차역전파법.assets/fig 5-21.png">

<img src="5장_오차역전파법.assets/fig 5-22.png">

```python
import numpy as np


class Sigmoid:
    def __init__(self):
        self.out = None

    def forward(self, x):
        # x : 입력
        # out : 출력 y
        out = 1 / (1 + np.exp(-x))
        self.out = out

        return out

    def backward(self, dout):6
        # dout : 역전파 상류층에서 내려온 값
        # self.out : 순전파의 출력 y
        dx = dout * self.out * (1 - self.out)

        return dx
```

## 5.6 Affine/Softmax 계층 구현하기

## 5.6.1 Affine 계층

신경망의 순전파 때 수행하는 행렬의 곱을 기하학에서는 `어파인 변환`이라고 한다. 그래서 어파인 변환을 수행하는 처리를 `Affine 계층`이라고 부른다.

행렬의 곱에서는 `대응하는 차원의 원소 수를 일치`시키는게 핵심이다. 

> Affine 계층의 계산 그래프와 미분 수식

<img src="5장_오차역전파법.assets/fig 5-24.png">

<img src="5장_오차역전파법.assets/e 5.13.png">

식에서 `T`는 전치행렬을 뜻한다. 원래 행렬의 `(i,j)` 위치의 원소를 `(j,i)` 위치의 원소로 바꾼 행렬을 뜻한다.

> 전치행렬을 반대항에 곱해주는 이유는 좌항과 우항의 형상`shape`를 맞추기 위해서가 아닐까 생각한다.

[참고 글](http://taewan.kim/post/backpropagation_matrix_transpose/)

> 전치행렬의 예

<img src="5장_오차역전파법.assets/e 5.14.png">

> Affine 계층의 역전파

<img src="5장_오차역전파법.assets/fig 5-25.png">

1과 2에서 각각 `교차되는 행렬`의 `전치행렬`을 상류에서 전파된 값에 곱해준다.

행렬의 곱에서는 형상`shape`를 항상 주의해야한다. 그러므로 `X`와 `X의 미분` 그리고 `W`와 `W의 미분` 이 같은 형상이라는 것을 기억해야한다.

만약 형상이 일치하지 않는다면 전치행렬을 사용하여 형상을 일치시켜야 할 수도 있다.

> X 의 형상과 X의 미분의 형상

<img src="5장_오차역전파법.assets/e 5.15.png">

### 5.6.2 배치용 Affine 계층

하나의 행렬이 아닌 데이터를 `N`개로 묶은 `배치용 Affine 계층`을 알아보자.

> 배치용 Affine 계층 계산 그래프

<img src="5장_오차역전파법.assets/fig 5-27.png">

기존과 다른 점은  `X`의 형상이 `(N,2)`가 됐다. 

편향`B`를 더할 때 주의할 점이 있다. 

순전파 때의 편향 덧셈은 `XㆍW` 에 대해 편향이 각 데이터에 더해진다.

```python
X_dot_W = np.array([[0, 0, 0], [10, 10, 10]])
B = np.array([1, 2, 3])

print(X_dot_W + B) # [[ 1  2  3] [11 12 13]]
```

데이터가 2개`N=2`일 때, 
편향의 역전파는 두 데이터에 대한 미분을 `데이터마다 더해서` 구해줘야하기 때문에 각 열의 합을 계산한다.

```python
dY = np.array([[1, 2, 3], [4, 5, 6]])
dB = np.sum(dY, axis=0)

print(dB) #[5 7 9]
```

> Affine 계층 코드

```python
import numpy as np


class Affine:
    def __init__(self, W, b):
        self.W = W
        self.b = b

        self.x = None
        self.dW = None
        self.db = None

    def forward(self, x):
        self.x = x
        out = np.dot(x, self.W) + self.b

        return out

    def backward(self, dout):
        dx = np.dot(dout, self.W.T)
        self.dW = np.dot(self.x.T, dout)
        self.db = np.sum(dout, axis=0)

        return dx
```

### 5.6.3 Softmax-with-Loss 계층

`Softmax`함수는 입력 값을 정규화해서 출력한다. Softmax 계층을 구현할 때에는 손실 함수인 `교차 엔트로피 오차`도 포함하여 구현한다.

> Softmax-with-Loss 계산 그래프와 간소화버전

<img src="5장_오차역전파법.assets/fig 5-29.png">

<img src="5장_오차역전파법.assets/fig 5-30.png">

`Softmax `계층의 순전파는 입력 `(a1,a2,a3)`를 정규화해서 `(y1,y2,y3)`를 출력한다. 

`Cross Entropy Error`은 출력 `(y1,y2,y3)`와 정답`(t1,t2,t3)`를 받고, 이 데이터들로부터 손실`L`을 출력한다.

`Softmax`계층의 역전파에서는 `(y1-t1, y2-t2, y3-t3)`를 출력한다. 정답`t`과 예측`y`의 `차분`을 출력하는 것이다.

---

예를들어 `t`가 `(0, 1, 0)`이고, `y`가 `(0.3, 0.2, 0.5)`일때를 생각해보자.

이 경우 `Softmax의 역전파`는 `(0.3, -0.8, 0.5)`라는 `큰 오차`를 하류로 전파한다. 결과적으로 `Softmax` 계층의 앞 계층들은 큰 오차로 인해 학습하는 정도도 커진다.

만약, `y`가 `(0.01, 0.99, 0)` 라면 `(0.01, -0.01, 0)`라는 `작은 오차`를 하류로 전파하고, 학습하는 정도가 작아진다.

> Softmax-with-Loss 계층 코드

```python
import numpy as np


def cross_entropy_error(y, t):
    # ndim : 배열의 차원 수
    # 만약 y의 차원이 1이라면 => [1 2 3]이라서 행과 열의 구분이 없다. shape = (3,)
    # 그래서 reshape를 통해 1행짜리 행렬로 변환시킨다. => [[1 2 3]] shape= (1,3)
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)

    if t.size == y.size:
        t = t.argmax(axis=1)
    batch_size = y.shape[0]
    # np.arrange : python의 range와 유사
    # y[np.arange(batch_size), t] :  batch_size 범위의 행마다의 t의 열
    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size


def softmax(x):
    if x.ndim == 2:
        x = x.T
        x = x - np.max(x, axis=0)
        y = np.exp(x) / np.sum(np.exp(x), axis=0)
        return y.T

    x = x - np.max(x)
    return np.exp(x) / np.sum(np.exp(x))


class SoftmaxWithLoss:
    def __init__(self):
        # 손실함수, 출력, 정답 레이블 정의
        self.loss = None
        self.y = None
        self.t = None

    def forward(self, x, t):
        # 정답레이블 저장
        self.t = t
        # 출력 저장
        self.y = softmax(x)
        # 손실함수 계산
        self.loss = cross_entropy_error(self.y, self.t)

        return self.loss

    def backward(self, dout=1):
        batch_size = self.t.shape[0]
        # 전파하는 값을 배치사이즈로 나눠서 데이터 1개당 오차를 앞 계층으로 전파
        dx = (self.y - self.t) / batch_size

        return dx

```



 ## 5.7 오차역전파법 구현하기

### 5.7.1  신경망 학습의 전체그림

- 전제 : `가중치`와 `편향`이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 `학습`이라고 한다.

1. 미니배치 : 훈련 데이터 중 일부를 `무작위`로 가져온다. 미니배치의 `손실함수 ` 값을 줄이는 것이 목표이다.
2. `기울기 산출` : 손실함수 값을 줄이기 위해 각 가중치의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게하는 방향을 제시한다. 
3. 매개변수 갱신 : 가중치를 기울기 방향으로 아주 조금 갱신한다.
4. 반복

네 가지 단계에서 `오차역전파법`은 `기울기 산출`을 위해 사용하고 있다.

앞 장에서는 `수치 미분`을 사용하여 기울기를 구했다. `수치 미분`은 상대적으로 구현이 쉽지만 계산이 오래 걸렸다.

`오차역전파법`은 수치 미분보다 빠르게 기울기를 구할 수 있다.

### 5.7.2 오차역전파법을 적용한 신경망 구현

```python
class TwoLayerNet:
    def __init__(
        self, input_size, hidden_size, output_size, weight_init_std=0.01
    ):
        self.params = {}
        self.params["W1"] = weight_init_std * np.random.randn(
            input_size, hidden_size
        )
        self.params["b1"] = np.zeros(hidden_size)
        self.params["W2"] = weight_init_std * np.random.randn(
            hidden_size, output_size
        )
        self.params["b2"] = np.zeros(output_size)

        # 계층 생성
        self.layers = dict()
        self.layers["Affine1"] = Affine(self.parmas["W1"], self.params["b1"])
        self.layers["Relu1"] = ReLU()
        self.layers["Affine2"] = Affine(self.parmas["W2"], self.params["b2"])
        self.lastLayer = SoftmaxWithLoss()

    def predict(self, x):
        # layer를 추가한 순서대로 수행된다.
        # Affine1 -> Relu1 -> Affine2
        for layer in self.layers.values():
            x = layer.forward(x)

        return x

    def loss(self, x, t):
        y = self.predict(x)

        # SoftmaxWithLoss 순전파 수행
        # y : 출력(예측 값), t : 정답 레이블
        return self.lastLayer.forward(y, t)

    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        # 정답레이블이 1차원이 아니면
        # 정답레이블 중 가장 큰 값의 인덱스 반환
        if t.ndim != 1:
            t = np.argmax(t, axis=1)

        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy

    # 오차역전파를 이용한 기울기 계산
    def gradient(self, x, t):
        # 순전파
        self.loss(x, t)

        # 역전파
        dout = 1
        dout = self.lastLayer.backward(dout)

        # 레이어 순서를 역순으로 구성해서 역전파 구현
        layers = list(self.layers.values())
        layers.reverse()
        for layer in layers:
            dout = layer.backward(dout)

        # 각 계층 기울기 저장
        grads = {}
        grads["W1"] = self.layers["Affine1"].dW
        grads["b1"] = self.layers["Affine1"].db
        grads["W2"] = self.layers["Affine2"].dW
        grads["b2"] = self.layers["Affine2"].db

        return grads
```

### 5.7.3. 오차역전파법으로 구한 기울기 검증하기

`수치 미분`과 `오차역전파법`으로 구한 기울기를 비교해서 `오차역전파법`의 구현이 제대로 되었는지 검증하는 방법

```python
import sys, os

sys.path.append(os.pardir)
import numpy as np
from two_layer_net import *
from dataset.mnist import *

(x_train, t_train), (x_test, t_test) = load_mnist(
    normalize=True, one_hot_label=True
)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

x_batch = x_train[:3]
t_batch = t_train[:3]

# 수치 미분을 이용한 기울기
grad_numerical = network.numerical_gradient(x_batch, t_batch)
# 오차역전파법을 이용한 기울기
grad_backprop = network.gradient(x_batch, t_batch)

# 각 가중치(W,b)의 수치 미분 기울기 - 오차역전파법 기울기 차이 평균
for key in grad_numerical.keys():
    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))
    print(key + " : " + str(diff))
    
# W1 : 2.465341946895199e-13
# b1 : 8.17287713200665e-13
# W2 : 9.325583661410133e-13
# b2 : 1.2012613265222071e-10

# 결과값을 보면 두 방법의 오차가 매우 적은 것을 확인할 수 있다.
# 오차역전파법이 제대로 구현된 것을 알 수 있다.
```

