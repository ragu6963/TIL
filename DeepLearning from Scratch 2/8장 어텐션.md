# 8장 어텐션 [github](https://github.com/WegraLee/deep-learning-from-scratch-2)

## 8.1 어텐션의 구조

**`어텐션 메커니즘`**덕분에 seq2seq가 필요한 정보에만 주목할 수 있게 된다.

### 8.1.1 seq2seq의 문제점

Encoder의 출력은 **`고정 길이 벡터`**는 큰 문제가 있다.

✅입력 문장의 길이에 상관없이 항상 같은 길이의 벡터로 변환하게되면 긴 문장의 경우 모든 정보를 벡터에 담을 수 없게된다.

<img src='assets/8장 어텐션/fig 8-1.png'>

### 8.1.2 Encoder 개선

✅ 문제점을 개선하기 위해 **`시각별(단어별)`**로 은닉 상태 벡터를 저장해서 Encoder의 출력 길이를 입력 문장 길이와 동일하게 하는 것이 좋다. 

여기서 각 은닉 상태에는 바로 전에 입력된 단어에 대한 정보가 많이 포함되어 있을 것이다. 

아래 그림에서 "고양이" 벡터에는 "나", "는", "고양이" 에 대한 정보가 담겨있지만 특히 "고양이"에 대한 정보가 많이 담겨있을 것이다.

<img src='assets/8장 어텐션/fig 8-2.png'>

### 8.1.3 Decoder 개선 1

앞장까지는 마지막 은닉 상태 벡터만 Decoder에서 사용하였다.

<img src='assets/8장 어텐션/fig 8-5.png'>

✅ 시각별 은닉 상태를  사용하기 위해서는 새로운 계산 계층이 추가된다. 

이 계산 계층은 (Encoder의 ) 시각별 은닉 상태 벡터와 (Decoder 의) 각 시각별 LSTM의 출력을 입력을 받고, 
Affine 계층으로 출력한다.

<img src='assets/8장 어텐션/fig 8-6.png'>

✅ 위 신경망의 목적은 **`얼라이먼트(단어의 대응관계를 나타내는 정보)`** 추출이다.

즉, Decoder가 I 출력할 때, hs에서 "나"에 대응하는 벡터를 선택하겠다는 의미이다. 
그리고 이 선택 작업은 어떤 계산 계층에서 이루어진다.

그런데 선택하는 작업은 미분할 수 없다는 문제가 있다. 

✅ 그래서 `모든 것을 선택`해서 `가중치`를 계산해서 원하는 벡터를 구한다.

각 단어의 중요도는 `가중치(a)`로 나타내고, hs 와 a의 가중치합을 구하여 원하는 벡터(가중치 합)를 얻는다.

<img src='assets/8장 어텐션/fig 8-7.png'>

<img src='assets/8장 어텐션/fig 8-8.png'>

> 가중치 합 계산 구현

```python
import numpy as np

T, H = 5, 4
hs = np.random.randn(T, H)
a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])

# 그림 8-9
# (1,5)행렬을 (5,1)로 변환하고, (5,1) 행렬을 4개 복사해서 (5,4) 행렬을 만든다.
ar = a.reshape(5,1).repeat(4, axis=1)
print(ar.shape) # (5, 4)

# 가중치 곱
t = hs * ar
print(t.shape) # (5, 4)

# 가중치 합
c = np.sum(t, axis=0) 
print(c.shape) # (1,4)
```

<img src='assets/8장 어텐션/fig 8-9.png'>

> 미니배치 처리용 가중치 합

```python
import numpy as np

N, T, H = 10, 5, 4
hs = np.random.randn(N, T, H)
a = np.random.randn(N, T)
ar = a.reshape(N, T, 1).repeat(H, axis=2) 

t = hs * ar
print(t.shape) # (10, 5, 4)

c = np.sum(t, axis=1) 
print(c.shape) # (10, 1, 4)
```

<img src='assets/8장 어텐션/fig 8-11.png'>

✅ `Repeate` 노드로 a를 복제하고, hs와 원소별 곱을 계산한 후 `Sum` 노드로 가중치 합을 구한다.

> 계층 구현

```python
class WeightSum:
    def __init__(self):
        self.params, self.grads = [], []
        self.cache = None

    def forward(self, hs, a):
        N, T, H = hs.shape

        ar = a.reshape(N, T, 1)
        # repeate 대신 넘파이의 브로드캐스트 이용
        t = hs * ar
        c = np.sum(t, axis=1)

        self.cache = (hs, ar)
        return c

    def backward(self, dc):
        hs, ar = self.cache
        N, T, H = hs.shape
        dt = dc.reshape(N, 1, H).repeat(T, axis=1)
        dar = dt * hs
        dhs = dt * ar
        da = np.sum(dar, axis=2)

        return dhs, da
```

### 8.1.4 Decoder 개선 2

✅ 가중치 **`a`**를 구하기 위해서는 내적과 소프트 맥스를 사용하면 된다.

<img src='assets/8장 어텐션/fig 8-12.png'>

Decoder의 LSTM 계층의 은닉 상태 벡터 `h` 와 `hs`의 각 단어 벡터의 유사도는 `내적`으로 계산한다.

> 내적은 두 벡터가 얼마나 같은 방향을 향하고 있는가를 의미한다. 그러므로 유사도를 나타내기 좋은 방법이다. 

<img src='assets/8장 어텐션/fig 8-13.png'>

<img src='assets/8장 어텐션/fig 8-14.png'>

두 벡터의 내적의 결과는 `s` 이고, 이 s 는 정규화하기 전의 값이다. 정규화 하기 위해 `Softmax` 함수를 적용한다.

<img src='assets/8장 어텐션/fig 8-15.png'>

> 계층 구현 

```python
# https://github.com/WegraLee/deep-learning-from-scratch-2/blob/master/ch08/attention_layer.py
import sys
sys.path.append('..')
import numpy as np
from common.layers import Softmax


class AttentionWeight:
    def __init__(self):
        self.params, self.grads = [], []
        self.softmax = Softmax()
        self.cache = None

    def forward(self, hs, h):
        N, T, H = hs.shape

        hr = h.reshape(N, 1, H)
        t = hs * hr 
        s = np.sum(t, axis=2)
        a = self.softmax.forward(s)

        self.cache = (hs, hr)
        return a

    def backward(self, da):
        hs, hr = self.cache
        N, T, H = hs.shape

        ds = self.softmax.backward(da)
        dt = ds.reshape(N, T, 1).repeat(H, axis=2)
        dhs = dt * hr
        dhr = dt * hs
        dh = np.sum(dhr, axis=1)

        return dhs, dh
```

### 8.1.5 Decoder 개선 3

✅ **`Weight Sum 과 Attention Weight`** 계층을 하나로 결합해보자

<img src='assets/8장 어텐션/fig 8-16.png'>

<img src='assets/8장 어텐션/fig 8-17.png'>

```python
# https://github.com/WegraLee/deep-learning-from-scratch-2/blob/master/ch08/attention_layer.py

class Attention:
    def __init__(self):
        self.params, self.grads = [], []
        # AttentionWeight 계층과 WeightSum 계층
        self.attention_weight_layer = AttentionWeight()
        self.weight_sum_layer = WeightSum()
        # 가중치 a
        self.attention_weight = None

    def forward(self, hs, h):
        # 가중치 a 계산
        a = self.attention_weight_layer.forward(hs, h)
        # 가중치 합 계산
        out = self.weight_sum_layer.forward(hs, a)
        # 가중치 a 저장
        self.attention_weight = a
        return out

    def backward(self, dout):
        dhs0, da = self.weight_sum_layer.backward(dout)
        dhs1, dh = self.attention_weight_layer.backward(da)
        dhs = dhs0 + dhs1
        return dhs, dh
```

### 8.2 어텐션을 갖춘 seq2seq 구현

### 8.2.1 Encoder 구현

✅ 이번 장의 Encoder 클래스의 forward() 메서드는 모든 은닉 상태를 반환한다.

```python
# https://github.com/WegraLee/deep-learning-from-scratch-2/blob/master/ch08/attention_seq2seq.py
import sys
sys.path.append('..')
from common.time_layers import *
from ch07.seq2seq import Encoder, Seq2seq
from ch08.attention_layer import TimeAttention


class AttentionEncoder(Encoder):
    def forward(self, xs):
        xs = self.embed.forward(xs)
        hs = self.lstm.forward(xs)
        return hs
    
    """
    이전 장 Encoder 순전파
    def forward(self, xs): 
        xs = self.embed.forward(xs)
        hs = self.lstm.forward(xs)
        self.hs = hs
        return hs[:, -1, :]
    """

    def backward(self, dhs):
        dout = self.lstm.backward(dhs)
        dout = self.embed.backward(dout)
        return dout

```

### 8.2.2 Decoder 구현

✅ Softmax 계층 이전까지의 Decoder를 구현해보자. 순전파와 역전파 그리고 새로운 단어열을 생성하는 메서드를 추가한다.

```python
# https://github.com/WegraLee/deep-learning-from-scratch-2/blob/master/ch08/attention_seq2seq.py
import sys
sys.path.append('..')
from common.time_layers import *
from ch07.seq2seq import Encoder, Seq2seq
from ch08.attention_layer import TimeAttention

class AttentionDecoder:
    def __init__(self, vocab_size, wordvec_size, hidden_size):
        V, D, H = vocab_size, wordvec_size, hidden_size
        rn = np.random.randn

        embed_W = (rn(V, D) / 100).astype('f')
        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')
        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')
        lstm_b = np.zeros(4 * H).astype('f')
        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')
        affine_b = np.zeros(V).astype('f')

        self.embed = TimeEmbedding(embed_W)
        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)
        # Attention 계층
        self.attention = TimeAttention()
        self.affine = TimeAffine(affine_W, affine_b)
        layers = [self.embed, self.lstm, self.attention, self.affine]

        self.params, self.grads = [], []
        for layer in layers:
            self.params += layer.params
            self.grads += layer.grads

    def forward(self, xs, enc_hs):
        h = enc_hs[:,-1]
        self.lstm.set_state(h)

        out = self.embed.forward(xs)
        dec_hs = self.lstm.forward(out)
        c = self.attention.forward(enc_hs, dec_hs)
        out = np.concatenate((c, dec_hs), axis=2)
        score = self.affine.forward(out)

        return score

    def backward(self, dscore):
        dout = self.affine.backward(dscore)
        N, T, H2 = dout.shape
        H = H2 // 2

        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]
        denc_hs, ddec_hs1 = self.attention.backward(dc)
        ddec_hs = ddec_hs0 + ddec_hs1
        dout = self.lstm.backward(ddec_hs)
        dh = self.lstm.dh
        denc_hs[:, -1] += dh
        self.embed.backward(dout)

        return denc_hs

    def generate(self, enc_hs, start_id, sample_size):
        sampled = []
        sample_id = start_id
        h = enc_hs[:, -1]
        self.lstm.set_state(h)

        for _ in range(sample_size):
            x = np.array([sample_id]).reshape((1, 1))

            out = self.embed.forward(x)
            dec_hs = self.lstm.forward(out)
            c = self.attention.forward(enc_hs, dec_hs)
            out = np.concatenate((c, dec_hs), axis=2)
            score = self.affine.forward(out)

            sample_id = np.argmax(score.flatten())
            sampled.append(sample_id)

        return sampled
```

### 8.2.3 seq2seq 구현

```python
# https://github.com/WegraLee/deep-learning-from-scratch-2/blob/master/ch08/attention_seq2seq.py

class AttentionSeq2seq(Seq2seq):
    def __init__(self, vocab_size, wordvec_size, hidden_size):
        args = vocab_size, wordvec_size, hidden_size
        # Attention 인코더와 디코더
        self.encoder = AttentionEncoder(*args)
        self.decoder = AttentionDecoder(*args)
        # Softmax 계층
        self.softmax = TimeSoftmaxWithLoss()

        self.params = self.encoder.params + self.decoder.params
        self.grads = self.encoder.grads + self.decoder.grads
```

## 8.3 어텐션 평가

✅ 날짜형식을 변경하는 문제로 **`어텐션 seq2seq`**을 평가해보자.

