# 2장 자연어와 단어의 분산표현

💡 `자연어 처리`의 본질적 문제는 컴퓨터가 우리의 말을 이해하게 만드는 것이다.

## 2.1 자연어 처리란

`자연어`란 한국어와 영어 등 우리가 평소에 쓰는 말을 뜻한다.

그래서 `자연어 처리(NLP)` 는 `자연어를 치리하는 기술`이다.

일반적인 프로그래밍 언어는 기계적이고 고정되어 있다. 즉, `딱딱한 언어`란 뜻이다.

하지만, 똑같은 의미의 문장도 여러 형태로 표현할 수 있는 자연어는 `부드러운 언어`이다.

그렇기 때문에 딱딱한 컴퓨터에게 부드러운 자연어를 이해하게 만드는 것은 매우 어렵다.

### 2.1.1 단어의 의미

컴퓨터에게 `단어의 의미`를 이해시키기 위한 방법 세가지를 살펴볼 것이다.

- 시소러스(유의어 사전)를 활용한 기법
- 통계 기반 기법
- 신경망을 활용한 추론 기반 기법(word2vec)

## 2.2 시소러스

`단어의 의미`를 나타내는 방법으로는 직접 단어의 의미를 정의하는 방식을 생각할 수 있다.

즉, `사전`처럼 각각의 단어에 의미를 설명해 넣을 수 있다. 

예를들어, 사전에서 `자동차`라는 단어를 찾으면 그 정의를 찾을 수 있다. 이런식으로 단어들을 정의해두면 컴퓨터도 단어의 의미를 이해할 수 있을지도 모른다.

그래서 사람들은 `시소러스` 형태로 사전을 만들려고 시도해왔다.

시소러스란 유의어 사전으로, `뜻이 같은 단어(동의어)나 뜻이 비슷한 단어(유의어)`가 한 그룹으로 분류되어 있다.

<img src="2장 자연어와 단어의 분산표현.assets/fig 2-1.png">

또한, 시소러스에서는 단어 사이의 `상위와 하위 혹은 전체와 부분` 등 더 세세한 관계까지 정의해둔 경우가 있다.

<img src="2장 자연어와 단어의 분산표현.assets/fig 2-2.png">

이처럼 모든 단어에 대한 유의어 집합을 만든 다음, 단어들의 관계를 `그래프`로 표현하여 ` 단어 사이의 연결`을 정의할 수 있다.

이 `그래프`로 컴퓨터에게 단어 사이의 관계를 가르칠 수 있다.

### 2.2.1 WordNet

자연어 처리에서 가장 유명한 시소러스가 `WordNet`이다.

### 2.2.2 시소러스의 문제점

1. 시대변화에 대응하기 어렵다.

   시대에 따라 언어의 의미가 변하기도 한다. 그래서 의미 변화에 대응하려면 시소러스를 끊임없이 갱신해야 한다.

2. 사람을 쓰는 비용이 크다.

   수작업으로 관계를 정의해야하기 때문에 큰 인적 비용이 발생한다.

3. 단어의 미묘한 차이를 표현할 수 없다.

   예를들면 빈티지와 레트로는 의미가 같지만 용법이 다르다. 시소러스에서는 이런 차이를 표현할 수 없다.

## 2.3 통계 기반 기법

통계 기반 기법을 살펴보기 위해 `말뭉치`를 이용할 것이다. `말뭉치`란 `대량의 텍스트 데이터` 이다.

다만, 맹목적으로 수집된 데이터가 아닌 자연어 처리 연구나 애플리케이션을 위해 수집된 데이터를 `말뭉치`라고 한다. 

`통계 기반 기법`은 이러한 말뭉치에서 자동으로 그리고 효율적으로 핵심을 추출하는 방법이다.

### 2.3.1 파이썬으로 말뭉치 전처리하기

> 말뭉치를 위한 사전 준비

```python
text = "You say goodbye and I say hello."
text = text.lower() # 모든 문자 소문자로

text = text.replace(".", " .")
print(text)  # 결과 :  you say goodbye and i say hello .

words = text.split(" ") # 문장을 단어별(공백 기준)로 분할
print(words)  # 결과 : ['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '.']

# 단어에 id 부여
word_to_id = {}

# id에 단어 부여
id_to_word = {}
 
for word in words:
    if word not in word_to_id:
        # id 값 생성
        new_id = len(word_to_id)

        # {단어 : id}
        word_to_id[word] = new_id

        # {id : 단어}
        id_to_word[new_id] = word

print(word_to_id) # {'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}
print(id_to_word) # {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}

print(id_to_word[2])  # goodbye
print(word_to_id["you"])  # 0

# id 목록 생성
corpus = [word_to_id[w] for w in words]
corpus = np.array(corpus)
print(corpus)  # [0 1 2 3 4 1 5 6]
```

### 2.3.2 단어의 분산 표현

단어의 `분산 표현`이란 `단어의 의미`를 마치 색을 `(R, G, B)` 로 표현하듯 벡터로 표현하는 것을 말한다.

예를들어 비색이 어떤 색인지 정확하게 몰라도 `(R, G, B) = (170, 33, 22)`라고 하면 빨강 계열의 색인란걸 알 수 있다.

### 2.3.3 분포 가설

단어를 벡터로 표현하는 중요한 기법의 대부분이 하나의 아이디어에서 나왔다.

그 아이디어는 `단어의 의미는 주변 단어에 의해 형성된다`라는 것이다.

이것을 `분포 가설`이라고 하며, 최근 연구도 대부분 이 가설에 기초한다.

---

`분포 가설`이 뜻하는 것은 단어 자체에는 의미가 없지만 단어가 사용된 `맥락(context)`이 의미를 형성한다는 것이다.

예를들면 `I drink beer와 We drink wine` 처럼 `drink` 주변에는 음료가 많이 등장할 것이다.

또한, `I guzzle beer와 We guzzle wine`이라는 문장이 있으면 `drink(마시다)와 guzzle(폭음하다)`는 의미가 가까운 단어라는 것을 알 수 있다.

여기서 `맥락`은 (주목하는 단어`예컨데 drink`) 주변에 놓인 단어를 가리킨다.

<img src="2장 자연어와 단어의 분산표현.assets/fig 2-3.png">

여기서 맥락의 크기를 `윈도우 크기`라고 한다. 윈도우 크기가 1이면 좌우 한 단어씩, 크기가 2이면 좌우 두 단어씩이 맥락에 포함된다.

### 2.3.4 동시발생 행렬

어떤 단어에 주목했을 때, 그 주변 단어를 세서 집계하는 방법을 `통계 기반`기법이라고 한다.

> You say goodbye and I say hello. 에 대한 동시발생 행렬 계산 과정 / 윈도우 크기 1

1. 단어를 정하고, 좌 우 윈도우 크기 만큼 단어의 수를 센다.

<img src="2장 자연어와 단어의 분산표현.assets/fig 2-4.png">

2. 벡터에 표시한다.

<img src="2장 자연어와 단어의 분산표현.assets/fig 2-5.png">

3. 단어 별로 반복

__결과__

<img src="2장 자연어와 단어의 분산표현.assets/fig 2-7.png">

> 함수 구현

```python
import sys

sys.path.append("..")
import numpy as np

# 전처리 함수
from common.util import preprocess


# 텍스트 생성
text = "You say goodbye and I say hello."

# 텍스트 전처리
corpus, word_to_id, id_to_word = preprocess(text)

vocab_size = len(word_to_id)

# 각 단어의 맥락에 해당하는 단어의 빈도 집계
# 단어 ID리스트, 어휘 수, 윈도우 크기
def create_co_matrix(corpus, vocab_size, window_size=1):
    # 행렬 사이즈 생성
    corpus_size = len(corpus)
    # 단어 빈도 저장용 행렬 새성
    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)

    for idx, word_id in enumerate(corpus):
        # 윈도우 사이즈만큼 탐색
        for i in range(1, window_size + 1):
            # 좌우 단어 인덱스
            left_idx = idx - 1
            right_idx = idx + 1

            # 좌측 단어 빈도 세기
            if left_idx >= 0:
                # 좌측 단어 id
                left_word_id = corpus[left_idx]
                # 좌측 단어 빈도 1증가
                co_matrix[word_id, left_word_id] += 1

            # 우측 단어 빈도 세기
            if right_idx < corpus_size:
                right_word_id = corpus[right_idx]
                co_matrix[word_id, right_word_id] += 1

    # 동시발생 행렬 반환
    return co_matrix


co_matrix = create_co_matrix(corpus, vocab_size, window_size=1)
print(co_matrix)
"""
[[0 1 0 0 0 0 0]
 [1 0 1 0 1 1 0]
 [0 1 0 1 0 0 0]
 [0 0 1 0 1 0 0]
 [0 1 0 1 0 0 0]
 [0 1 0 0 0 0 1]
 [0 0 0 0 0 1 0]]
"""
```

### 2.3.5 벡터 간 유사도

단어 벡터 간 유사도를 구할 때에는 `코사인 유사도`를 자주 사용한다.

두 벡터 `X = (x1,x2,...,xn)과 Y = (y1,y2,...,yn)` 이 있다면 코사인 유사도는 다음 식으로 정의된다. 

<img src="2장 자연어와 단어의 분산표현.assets/e 2-1.png">

분자에는 벡터의 `내적`, 분모에는 각 벡터의 `노름`으로 표현한다.

여기서는 `L2 노름`을 계산하는데 L2 노름은 벡터의 각 원소를 제곱해 더한 후 다시 제곱근을 하는 것이다.

위 식의 핵심은 벡터를 정규화하고 내적을 구하는 것이다.

> 두 벡터의 방향이 완전히 같으면 코사인 유사도는 1, 반대라면 -1이 된다.

> 코사인 유사도 구현

```python
import numpy as np


def cos_similarity(x, y):
    nx = x / np.sqrt(np.sum(x ** 2))  # x의 정규화
    ny = y / np.sqrt(np.sum(y ** 2))  # y의 정규화
    return np.dot(nx, ny)  # 내적
"""
만약 제로 벡터가 인수로 들어오면 0으로 나누는 오류가 발생한다.
"""
```

```python
# 0으로 나누는 오류 방지 코드
# 각 원소에 아주 작은 값(1e-8) 을 더해준다
import numpy as np


def cos_similarity(x, y, eps=1e-8):
    nx = x / np.sqrt(np.sum(x ** 2) + eps)  # x의 정규화
    ny = y / np.sqrt(np.sum(y ** 2) + eps)  # y의 정규화
    return np.dot(nx, ny)  # 내적
```

> you 와 i의 유사도

```python
import sys

sys.path.append("..")

from common.util import preprocess, create_co_matrix, cos_similarity

text = "You say goodbye and I say hello."

corpus, word_to_id, id_to_word = preprocess(text)

vocab_size = len(word_to_id)

# 동시발생 행렬 생성
C = create_co_matrix(corpus, vocab_size)

c0 = C[word_to_id["you"]]  # you의 단어 벡터
c1 = C[word_to_id["i"]]  # i의 단어 벡터

# 코사인 유사도 계산
print(cos_similarity(c0, c1))  # 0.7071067691154799
```

### 2.3.6 유사 단어의 랭킹 표시

어떤 단어가 검색어로 주어지면, 그 검색어와 비슷한 단어를 유사도 순으로 출력하는 함수를 구현 해보자.

> you와 유사한 단어 5개 출력

```python
import sys
import numpy as np

sys.path.append("..")

from common.util import preprocess, create_co_matrix, cos_similarity


def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):
    # 검색어를 꺼낸다.
    if query not in word_to_id:
        print(f"{query}를 찾을 수 없습니다.")
        return

    print("\n[query] " + query)

    query_id = word_to_id[query]
    query_vec = word_matrix[query_id]

    # 코사인 유사도 계산
    vocab_size = len(id_to_word)
    similarity = np.zeros(vocab_size)
    for i in range(vocab_size):
        similarity[i] = cos_similarity(word_matrix[i], query_vec)

    # 코사인 유사도를 기준으로 내림차순으로 출력
    count = 0
    # argsort : 넘파이 배열의 원소를 오름차순으로 정렬 단, 반환값은 배열의 인덱스
    for i in (-1 * similarity).argsort():
        # 검색 단어와 같은 단어라면 pass
        if id_to_word[i] == query:
            continue

        # 단어 : 검색 단어와의 유사도
        print(" %s: %s" % (id_to_word[i], similarity[i]))

        count += 1
        # 출력 개수 넘어가면 종료
        if count >= top:
            return


text = "You say goodbye and I say hello."
corpus, word_to_id, id_to_word = preprocess(text)
vocab_size = len(word_to_id)
C = create_co_matrix(corpus, vocab_size)

most_similar("you", word_to_id, id_to_word, C, top=5)
"""
[query] you
 goodbye: 0.7071067691154799
 i: 0.7071067691154799
 hello: 0.7071067691154799
 say: 0.0
 and: 0.0
"""
```

## 2.4 통계 기반 기법 개선하기

### 2.4.1 상호정보량

`동시발생 행렬`의 원소는 두 단어가 동시에 발생한 횟수를 나타낸다.

그러나 이것은 그리 좋은 특징이 아니다.

예를들어 말뭉치에서 `the`와 `car`의 동시발생을 살펴보면 "... the car ..."라는 문구로 인해 두 단어의 동시발생 횟수는 아주 많을 것이다.

한편, `car`와 `drive`는 관련이 깊지만 등장횟수로만 보면 `car`는 `drive`보다 `the`보다 더 관련성이 강할 것이다.

이 문제를 해결하기 위해 `점별 상호정보량(PMI)`라는 척도를 사용한다.

`PMI`는 확률 변수 x와 y에 대해 다음 식으로 정의된다.

> P(x)는 x가 일어날 확률, P(y)는 y가 일어날 확률, P(x, y)는 x와 y가 동시에 일어날 확률 

<img src="2장 자연어와 단어의 분산표현.assets/e 2-2.png">

✨`PMI`가 높을수록 관련성이 높다는 의미이다.

위 식을 자연어에 적용하면 `P(X)`는 단어 x가 말뭉치에 등장할 확률이다.

예를들어 10000개의 단어로 이뤄진 말뭉치에서 "the"가 100번 등장하면 P("the") = 100/10000 = 0.01 이다.

10000개의 단어에서 "the"와 "car"가 10번 동시발생했다면 P("the","car") = 10/100000 = 0.001 이다.

위 식을 동시발생 행렬을 사용하여 표현하면 다음 식으로 표현할 수 있다.

> C(x) 와 C(y)는 각각 x와 y의 등장 횟수, C(x,y)는 x와 y가 동시발생하는 횟수 이다.

<img src="2장 자연어와 단어의 분산표현.assets/e 2-3.png">

> N = 10000, "the" "car" "drive" 가 각각 1000, 20, 10 등장하고, "the"와 "car", "car"와 "drive"의 동시 발생횟수가 각각 10, 5일 때의 PMI

<img src="2장 자연어와 단어의 분산표현.assets/e 2-4.png">

<img src="2장 자연어와 단어의 분산표현.assets/e 2-5.png">

`PMI`를 이용하면 "drive"가 "the"보다 "car"에 대한 관련성이 높게 나온다.

---

두 단어의 동시발생 횟수가 0이면 `log0 = 무한대`가 된다.

이 문제를 해결하기 위해 실제로는 `양의 상호정보량(PPMI)` 를 사용한다.

<img src="2장 자연어와 단어의 분산표현.assets/e 2-6.png">

> PPIM 구현

```python
def ppmi(C, verbose=False, eps=1e-8):
    """
    PPMI(양의 상호정보량)
    C : 동시발생 행렬
    verbose : 진행 상황을 출력할지 여부
    """

    # PPMI 결과 저장 행렬
    M = np.zeros_like(C, dtype=np.float32)

    # 말뭉치 단어 수
    N = np.sum(C)

    # 각 단어의 발생 횟수
    S = np.sum(C, axis=0)

    total = C.shape[0] * C.shape[1]
    cnt = 0

    for i in range(C.shape[0]):
        for j in range(C.shape[1]):
            # 인덱스 i 단어와 인덱스 j 단어의 PPMI 계산

            # PMI 계산
            # C[i, j] = i 단어와 j가 동시 발생할 확률
            pmi = np.log2(C[i, j] * N / (S[j] * S[i]) + eps)

            # PPMI
            M[i, j] = max(0, pmi)

            # 진행상황 표시
            if verbose:
                cnt += 1
                if cnt % (total // 100) == 0:
                    print("%.1f%% 완료" % (100 * cnt / total))
    return M
```

> 동시발생 행렬과 그에 대한 PPMI 결과

<img src="2장 자연어와 단어의 분산표현.assets/image-20210304214150280.png">

💣 `PPMI`의 단점

- 말뭉치의 단어 수가 증가하면 각 단어의 벡터의 차원 수도 증가한다. 즉, 단어수가 10만 개라면 벡터의 차원 수도 10만이 된다.
- 행렬 값의 대부분이 0이다. 즉, 각 원소의 중요도가 낮다는 뜻이다.
- 노이즈에 약하고, 견고하지 못하다.

이러한 단점을 대처하기 위해 자주 수행하는 기법이 `벡터의 차원 감소`이다.

### 2.4.2 차원 감소

`차원 감소`는 벡터의 차원을 줄이는 방법이다.

하지만 단순히 줄이는게 아니라 `중요한 정보`는 최대한 유지하는 것이 핵심이다.

아래 그림처럼 데이터의 분포를 고려해 중요한 `축`을 찾는 일을 수행한다.

<img src="2장 자연어와 단어의 분산표현.assets/fig 2-8.png">