# 2장 자연어와 단어의 분산표현

💡 `자연어 처리`의 본질은 컴퓨터가 우리의 말을 이해하게 만드는 것이다.

## 2.1 자연어 처리란

`자연어`란 한국어와 영어 등 우리가 평소에 쓰는 말을 뜻한다.

그래서 `자연어 처리(NLP)` 는 `자연어를 치리하는 기술`이다.

일반적인 프로그래밍 언어는 기계적이고 고정되어 있다. 즉, `딱딱한 언어`란 뜻이다.

하지만, 똑같은 의미의 문장도 여러 형태로 표현할 수 있는 자연어는 `부드러운 언어`이다.

그렇기 때문에 딱딱한 컴퓨터에게 부드러운 자연어를 이해하게 만드는 것은 매우 어렵다.

### 2.1.1 단어의 의미

컴퓨터에게 `단어의 의미`를 이해시키기 위한 방법 세가지를 살펴볼 것이다.

- 시소러스(유의어 사전)를 활용한 기법
- 통계 기반 기법
- 신경망을 활용한 추론 기반 기법(word2vec)

## 2.2 시소러스

`단어의 의미`를 나타내는 방법으로는 직접 단어의 의미를 정의하는 방식을 생각할 수 있다.

즉, `사전`처럼 각각의 단어에 의미를 설명해 넣을 수 있다. 

예를들어, 사전에서 `자동차`라는 단어를 찾으면 그 정의를 찾을 수 있다. 이런식으로 단어들을 정의해두면 컴퓨터도 단어의 의미를 이해할 수 있을지도 모른다.

그래서 사람들은 `시소러스` 형태로 사전을 만들려고 시도해왔다.

시소러스란 유의어 사전으로, `뜻이 같은 단어(동의어)나 뜻이 비슷한 단어(유의어)`가 한 그룹으로 분류되어 있다.

<img src="2장 자연어와 단어의 분산표현.assets/fig 2-1.png">

또한, 시소러스에서는 단어 사이의 `상위와 하위 혹은 전체와 부분` 등 더 세세한 관계까지 정의해둔 경우가 있다.

<img src="2장 자연어와 단어의 분산표현.assets/fig 2-2.png">

이처럼 모든 단어에 대한 유의어 집합을 만든 다음, 단어들의 관계를 `그래프`로 표현하여 ` 단어 사이의 연결`을 정의할 수 있다.

이 `그래프`로 컴퓨터에게 단어 사이의 관계를 가르칠 수 있다.

### 2.2.1 WordNet

자연어 처리에서 가장 유명한 시소러스가 `WordNet`이다.

### 2.2.2 시소러스의 문제점

1. 시대변화에 대응하기 어렵다.

   시대에 따라 언어의 의미가 변하기도 한다. 그래서 의미 변화에 대응하려면 시소러스를 끊임없이 갱신해야 한다.

2. 사람을 쓰는 비용이 크다.

   수작업으로 관계를 정의해야하기 때문에 큰 인적 비용이 발생한다.

3. 단어의 미묘한 차이를 표현할 수 없다.

   예를들면 빈티지와 레트로는 의미가 같지만 용법이 다르다. 시소러스에서는 이런 차이를 표현할 수 없다.

## 2.3 통계 기반 기법

통계 기반 기법을 살펴보기 위해 `말뭉치`를 이용할 것이다. `말뭉치`란 `대량의 텍스트 데이터` 이다.

다만, 맹목적으로 수집된 데이터가 아닌 자연어 처리 연구나 애플리케이션을 위해 수집된 데이터를 `말뭉치`라고 한다. 

`통계 기반 기법`은 이러한 말뭉치에서 자동으로 그리고 효율적으로 핵심을 추출하는 방법이다.

### 2.3.1 파이썬으로 말뭉치 전처리하기

> 말뭉치를 위한 사전 준비

```python
text = "You say goodbye and I say hello."
text = text.lower() # 모든 문자 소문자로

text = text.replace(".", " .")
print(text)  # 결과 :  you say goodbye and i say hello .

words = text.split(" ") # 문장을 단어별(공백 기준)로 분할
print(words)  # 결과 : ['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '.']

# 단어에 id 부여
word_to_id = {}

# id에 단어 부여
id_to_word = {}
 
for word in words:
    if word not in word_to_id:
        # id 값 생성
        new_id = len(word_to_id)

        # {단어 : id}
        word_to_id[word] = new_id

        # {id : 단어}
        id_to_word[new_id] = word

print(word_to_id) # {'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}
print(id_to_word) # {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}

print(id_to_word[2])  # goodbye
print(word_to_id["you"])  # 0

# id 목록 생성
corpus = [word_to_id[w] for w in words]
corpus = np.array(corpus)
print(corpus)  # [0 1 2 3 4 1 5 6]
```

### 2.3.2 단어의 분산 표현

단어의 `분산 표현`이란 `단어의 의미`를 마치 색을 `(R, G, B)` 로 표현하듯 벡터로 표현하는 것을 말한다.

예를들어 비색이 어떤 색인지 정확하게 몰라도 `(R, G, B) = (170, 33, 22)`라고 하면 빨강 계열의 색인란걸 알 수 있다.

### 2.3.3 분포 가설

단어를 벡터로 표현하는 중요한 기법의 대부분이 하나의 아이디어에서 나왔다.

그 아이디어는 `"📌단어의 의미는 주변 단어에 의해 형성된다"`라는 것이다.

이것을 `분포 가설`이라고 하며, 최근 연구도 대부분 이 가설에 기초한다.

---

`분포 가설`이 뜻하는 것은 단어 자체에는 의미가 없지만 단어가 사용된 `맥락(context)`이 의미를 형성한다는 것이다.

예를들면 `I drink beer와 We drink wine` 처럼 `drink` 주변에는 음료가 많이 등장할 것이다.

또한, `I guzzle beer와 We guzzle wine`이라는 문장이 있으면 `drink(마시다)와 guzzle(폭음하다)`는 의미가 가까운 단어라는 것을 알 수 있다.

여기서 `맥락`은 (주목하는 단어`예컨데 drink`) 주변에 놓인 단어를 가리킨다.

<img src="2장 자연어와 단어의 분산표현.assets/fig 2-3.png">

여기서 맥락의 크기를 `📌 윈도우 크기`라고 한다. 윈도우 크기가 1이면 좌우 한 단어씩, 크기가 2이면 좌우 두 단어씩이 맥락에 포함된다.

### 2.3.4 동시발생 행렬

어떤 단어에 주목했을 때, 그 `📌주변 단어를 세서 집계`하는 방법을 `통계 기반`기법이라고 한다.

> You say goodbye and I say hello. 에 대한 동시발생 행렬 계산 과정 / 윈도우 크기 1

1. 단어를 정하고, 좌 우 윈도우 크기 만큼 단어의 수를 센다.

<img src="2장 자연어와 단어의 분산표현.assets/fig 2-4.png">

2. 벡터에 표시한다.

<img src="2장 자연어와 단어의 분산표현.assets/fig 2-5.png">

3. 단어 별로 반복

__결과__

<img src="2장 자연어와 단어의 분산표현.assets/fig 2-7.png">

> 함수 구현

```python
import sys

sys.path.append("..")
import numpy as np

# 전처리 함수
from common.util import preprocess


# 텍스트 생성
text = "You say goodbye and I say hello."

# 텍스트 전처리
# 단어 id 목록, 단어를 id로, id를 단어로 
corpus, word_to_id, id_to_word = preprocess(text)

vocab_size = len(word_to_id)

# 각 단어의 맥락에 해당하는 단어의 빈도 집계
# 단어 ID리스트, 어휘 수, 윈도우 크기
def create_co_matrix(corpus, vocab_size, window_size=1):
    # 행렬 사이즈 생성
    corpus_size = len(corpus)
    
    # 단어 빈도 저장용 행렬 새성
    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)

    for idx, word_id in enumerate(corpus):
        # 윈도우 사이즈만큼 탐색
        for i in range(1, window_size + 1):
            # 좌우 단어 인덱스
            left_idx = idx - 1
            right_idx = idx + 1

            # 좌측 단어 빈도 세기
            if left_idx >= 0:
                # 좌측 단어 id
                left_word_id = corpus[left_idx]
                # 좌측 단어 빈도 1증가
                co_matrix[word_id, left_word_id] += 1

            # 우측 단어 빈도 세기
            if right_idx < corpus_size:
                right_word_id = corpus[right_idx]
                co_matrix[word_id, right_word_id] += 1

    # 동시발생 행렬 반환
    return co_matrix


co_matrix = create_co_matrix(corpus, vocab_size, window_size=1)
print(co_matrix)
"""
[[0 1 0 0 0 0 0]
 [1 0 1 0 1 1 0]
 [0 1 0 1 0 0 0]
 [0 0 1 0 1 0 0]
 [0 1 0 1 0 0 0]
 [0 1 0 0 0 0 1]
 [0 0 0 0 0 1 0]]
"""
```

### 2.3.5 벡터 간 유사도

단어 벡터 간 유사도를 구할 때에는 `코사인 유사도`를 자주 사용한다.

두 벡터 `X = (x1,x2,...,xn)과 Y = (y1,y2,...,yn)` 이 있다면 코사인 유사도는 다음 식으로 정의된다. 

<img src="2장 자연어와 단어의 분산표현.assets/e 2-1.png">

분자에는 벡터의 `내적`, 분모에는 각 벡터의 `노름`으로 표현한다.

여기서는 `L2 노름`을 계산하는데 L2 노름은 벡터의 각 원소를 제곱해 더한 후 다시 제곱근을 하는 것이다.

위 식의 핵심은 벡터를 정규화하고 내적을 구하는 것이다.

> 두 벡터의 방향이 완전히 같으면 코사인 유사도는 1, 반대라면 -1이 된다.

> 코사인 유사도 구현

```python
import numpy as np


def cos_similarity(x, y):
    nx = x / np.sqrt(np.sum(x ** 2))  # x의 정규화
    ny = y / np.sqrt(np.sum(y ** 2))  # y의 정규화
    return np.dot(nx, ny)  # 내적
"""
만약 제로 벡터가 인수로 들어오면 0으로 나누는 오류가 발생한다.
"""
```

```python
# 0으로 나누는 오류 방지 코드
# 각 원소에 아주 작은 값(1e-8) 을 더해준다
import numpy as np


def cos_similarity(x, y, eps=1e-8):
    nx = x / np.sqrt(np.sum(x ** 2) + eps)  # x의 정규화
    ny = y / np.sqrt(np.sum(y ** 2) + eps)  # y의 정규화
    return np.dot(nx, ny)  # 내적
```

> you 와 i의 유사도

```python
import sys

sys.path.append("..")

from common.util import preprocess, create_co_matrix, cos_similarity

text = "You say goodbye and I say hello."

corpus, word_to_id, id_to_word = preprocess(text)

vocab_size = len(word_to_id)

# 동시발생 행렬 생성
C = create_co_matrix(corpus, vocab_size)

c0 = C[word_to_id["you"]]  # you의 단어 벡터
c1 = C[word_to_id["i"]]  # i의 단어 벡터

# 코사인 유사도 계산
print(cos_similarity(c0, c1))  # 0.7071067691154799
```

### 2.3.6 유사 단어의 랭킹 표시

어떤 단어가 검색어로 주어지면, 그 검색어와 비슷한 단어를 유사도 순으로 출력하는 함수를 구현 해보자.

> you와 유사한 단어 5개 출력

```python
import sys
import numpy as np

sys.path.append("..")

from common.util import preprocess, create_co_matrix, cos_similarity


def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):
    # 검색어를 꺼낸다.
    if query not in word_to_id:
        print(f"{query}를 찾을 수 없습니다.")
        return

    print("\n[query] " + query)

   	# 검색 단어의 id
    query_id = word_to_id[query]
    # 검색 단어의 동시발생 행렬 값
    query_vec = word_matrix[query_id]

    # 코사인 유사도 계산
    vocab_size = len(id_to_word)
    similarity = np.zeros(vocab_size)
    for i in range(vocab_size):
        similarity[i] = cos_similarity(word_matrix[i], query_vec)

    # 코사인 유사도를 기준으로 내림차순으로 출력
    count = 0
    # argsort : 넘파이 배열의 원소를 오름차순으로 정렬 단, 반환값은 배열의 인덱스
    for i in (-1 * similarity).argsort():
        # 검색 단어와 같은 단어라면 pass
        if id_to_word[i] == query:
            continue

        # 단어 : 검색 단어와의 유사도
        print(" %s: %s" % (id_to_word[i], similarity[i]))

        count += 1
        # 출력 개수 넘어가면 종료
        if count >= top:
            return


text = "You say goodbye and I say hello."

# 데이터 전처리
# 단어 id 목록, 단어를 id로, id를 단어로
corpus, word_to_id, id_to_word = preprocess(text)
vocab_size = len(word_to_id)

# 동시발생행렬 생성
C = create_co_matrix(corpus, vocab_size)

most_similar("you", word_to_id, id_to_word, C, top=5)
"""
[query] you
 goodbye: 0.7071067691154799
 i: 0.7071067691154799
 hello: 0.7071067691154799
 say: 0.0
 and: 0.0
"""
```

## 2.4 통계 기반 기법 개선하기

### 2.4.1 상호정보량

`동시발생 행렬`의 원소는 두 단어가 동시에 발생한 횟수를 나타낸다. 그러나 이것은 그리 좋은 특징이 아니다.

예를들어 말뭉치에서 `the`와 `car`의 동시발생을 살펴보면 "... the car ..."라는 문구로 인해 두 단어의 동시발생 횟수는 아주 많을 것이다.

한편, `car`와 `drive`는 관련이 깊지만 등장횟수로만 보면 `car`는 `drive`보다 `the`보다 더 관련성이 강할 것이다.

✨이 문제를 해결하기 위해 `점별 상호정보량(PMI)`라는 척도를 사용한다.

`PMI`는 확률 변수 x와 y에 대해 다음 식으로 정의된다.

> P(x)는 x가 일어날 확률, P(y)는 y가 일어날 확률, P(x, y)는 x와 y가 동시에 일어날 확률 

<img src="2장 자연어와 단어의 분산표현.assets/e 2-2.png">

---

✨`PMI`가 높을수록 관련성이 높다는 의미이다.

위 식을 자연어에 적용하면 `P(X)`는 단어 x가 말뭉치에 등장할 확률이다.

예를들어 10000개의 단어로 이뤄진 말뭉치에서 "the"가 100번 등장하면 P("the") = 100/10000 = 0.01 이다.

10000개의 단어에서 "the"와 "car"가 10번 동시발생했다면 P("the","car") = 10/100000 = 0.001 이다.

위 식을 동시발생 행렬을 사용하여 표현하면 다음 식으로 표현할 수 있다.

> C(x) 와 C(y)는 각각 x와 y의 등장 횟수, C(x,y)는 x와 y가 동시발생하는 횟수 이다.

<img src="2장 자연어와 단어의 분산표현.assets/e 2-3.png">

> N = 10000, "the" "car" "drive" 가 각각 1000, 20, 10 등장하고, "the"와 "car", "car"와 "drive"의 동시 발생횟수가 각각 10, 5일 때의 PMI

<img src="2장 자연어와 단어의 분산표현.assets/e 2-4.png">

<img src="2장 자연어와 단어의 분산표현.assets/e 2-5.png">

`PMI`를 이용하면 "drive"가 "the"보다 "car"에 대한 관련성이 높게 나온다.

---

두 단어의 동시발생 횟수가 0이면 `log0 = 무한대`가 된다.

이 문제를 해결하기 위해 실제로는 `양의 상호정보량(PPMI)` 를 사용한다.

<img src="2장 자연어와 단어의 분산표현.assets/e 2-6.png">

> PPIM 구현

```python
def ppmi(C, verbose=False, eps=1e-8):
    """
    PPMI(양의 상호정보량)
    C : 동시발생 행렬
    verbose : 진행 상황을 출력할지 여부
    """

    # PPMI 결과 저장 행렬
    M = np.zeros_like(C, dtype=np.float32)

    # 말뭉치 단어 수
    N = np.sum(C)

    # 각 단어의 발생 횟수
    S = np.sum(C, axis=0)

    total = C.shape[0] * C.shape[1]
    cnt = 0

    for i in range(C.shape[0]):
        for j in range(C.shape[1]):
            # 인덱스 i 단어와 인덱스 j 단어의 PPMI 계산

            # PMI 계산
            # C[i, j] = i 단어와 j가 동시 발생할 확률
            pmi = np.log2(C[i, j] * N / (S[j] * S[i]) + eps)

            # PPMI
            M[i, j] = max(0, pmi)

            # 진행상황 표시
            if verbose:
                cnt += 1
                if cnt % (total // 100) == 0:
                    print("%.1f%% 완료" % (100 * cnt / total))
    return M
```

> 동시발생 행렬과 그에 대한 PPMI 결과

<img src="2장 자연어와 단어의 분산표현.assets/image-20210304214150280.png">

💣 `PPMI`의 단점

- 말뭉치의 단어 수가 증가하면 각 단어의 벡터의 차원 수도 증가한다. 즉, 단어수가 10만 개라면 벡터의 차원 수도 10만이 된다.
- 행렬 값의 대부분이 0이다. 즉, 각 원소의 중요도가 낮다는 뜻이다.
- 노이즈에 약하고, 견고하지 못하다.

이러한 단점을 대처하기 위해 자주 수행하는 기법이 `벡터의 차원 감소`이다.

### 2.4.2 차원 감소

`차원 감소`는 벡터의 차원을 줄이는 방법이다.

하지만 단순히 줄이는게 아니라 `중요한 정보`는 최대한 유지하는 것이 핵심이다.

아래 그림처럼 데이터의 분포를 고려해 중요한 `축`을 찾는 일을 수행한다.

<img src="2장 자연어와 단어의 분산표현.assets/fig 2-8.png">

그림의 왼쪽은 데이터점들을 2차원 좌표에 표시한 것이고, 오른쪽은 새로운 축을 도입해 좌표축 하나만으로 표시한 것이다.

이때 각 데이터점의 값은 새로춘 축으로 사영된 값으로 변한다.`사영되다 : 빛이나 형상이 그대로 옮겨져 비추어지다`

> 즉 데이터의 차원을 줄인다(?)

중요한 것은 가장 적합한 축을 찾아내는 일로, 1차원 값만으로 데이터의 본질적인 차이를 구별할 수 있어야한다.

✨`특잇값분해(SVD)`로 차원을 감소시켜보자. 수식은 아래와 같다.

<img src="2장 자연어와 단어의 분산표현.assets/e 2-7.png">

`SVD`는 임의의 행렬 X를 U,S,V라는 세 행렬의 곱으로 분해한다.

`U, V`는 직교행렬이고, 그 열벡터는 서로 직교한다.`직교행렬 : 정방행렬로 열과 행들은 직교 단위 벡터가 된다. `

> 직교행렬

<img src="2장 자연어와 단어의 분산표현.assets/266C424558E5EF9126">

`S`는 대각행렬이다.`대각행렬 : 대각성분 외에는 모두 0인 행렬`

<img src="2장 자연어와 단어의 분산표현.assets/fig 2-9.png">

직교행렬인 U 는 어떠한 공간의 축을 형상한다. 현재 맥락에서는 U 행렬을 `단어 공간`으로 취급할 수 있다.

대각행렬인 S 는 성분에 `특잇값`이 큰 순서로 나열되어 있다. 특잇값이란, 쉽게 말해 `해당 축`의 중요도라고 간주할 수 있다.

그래서 아래 그림처럼 중요도가 낮은(특잇값이 작은 원소)를 깎아내는 방법을 생각할 수 있다.

 <img src="2장 자연어와 단어의 분산표현.assets/fig 2-10.png">대각행렬 S 에서 `특잇값이 작다면 중요도가 낮다는 뜻`이므로 직교행렬 U 에서 여분의 열벡터를 깎아내어 원래의 행렬을 근사할 수 있다.

---

`단어의 PPMI 행렬`에 적용하면 

행렬 X의 각 행에는 단어 ID의 단어 벡터가 저장되어 있고, 그 단어 벡터가 행렬 U'라는 `차원 감소된 벡터`로 표현되는 것이다 

### 2.4.3 SVD에 의한 차원 감소

> SVD 구현

```python
import sys

sys.path.append("..")
import numpy as np
import matplotlib.pyplot as plt
from common.util import preprocess, create_co_matrix, ppmi


text = "You say goodbye and I say hello."
corpus, word_to_id, id_to_word = preprocess(text)
print(word_to_id)
vocab_size = len(id_to_word)
C = create_co_matrix(corpus, vocab_size, window_size=1)
W = ppmi(C)

# SVD
U, S, V = np.linalg.svd(W)

np.set_printoptions(precision=3)  # 유효 자릿수를 세 자리로 표시
# 동시 발생행렬, [0 1 0 0 0 0 0]
print(C[0])

# PPMI 발생행렬, [0.    1.807 0.    0.    0.    0.    0.   ]
print(W[0])

# SVD 발생행렬, [-1.110e-16  3.409e-01 -1.205e-01 -4.163e-16 -1.110e-16 -9.323e-01 -2.426e-17]
print(U[0])

# 차원을 2차원 벡터로 줄일려면
# print(U[0, : 2])

for word, word_id in word_to_id.items():
    plt.annotate(word, (U[word_id, 1], U[word_id, 0]))
    
# scatter(x축 값, y축 값)
# U[:, 1] : 2열 전체값, U[:, 0] : 1열 전체값
# 차원을 2차원으로 줄였다
plt.scatter(U[:, 1], U[:, 0], alpha=0.5)
plt.show()

```

<img src="2장 자연어와 단어의 분산표현.assets/fig 2-11.png">

```
SVD의 시간 복잡도는 O(N^3, N : 행렬의 크기)이다.
현실적으로는 사용이 어렵기 떄문에 Truncated SVD 라는 떠 빠른 기법을 이용한다.
```

### 2.4.4 PTB 데이터셋

`펜 트리뱅크(PTB)`는 기법의 품질을 측정하는 벤치마크로 자주 이용되는 말뭉치이다.

PTB에는 몇가지 특징이 있다.

- 희소한 단어를 <unk> 로 치환한다(unk : unknown의 약어)
- 구체적인 숫자를 "N" 으로 대체한다.
- 한 문장이 하나의 줄로 저장되어 있다.

> PTB 말뭉치 확인 코드

```python
import sys

sys.path.append("..")
from dataset import ptb
# ptb 모듈 주소
# https://github.com/WegraLee/deep-learning-from-scratch-2/blob/master/dataset/ptb.py

# corpus : 단어 ID 목록
# word_to_id : 단어를 ID로
# id_to_word : ID를 단어로

# train: 훈련용, test : 테스트용, valid: 검증용
corpus, word_to_id, id_to_word = ptb.load_data("train")

print("말뭉치 크기:", len(corpus))
print("corpus[:30]:", corpus[:30])
print()
print("id_to_word[0]:", id_to_word[0])
print("id_to_word[1]:", id_to_word[1])
print("id_to_word[2]:", id_to_word[2])
print()
print("word_to_id['car']:", word_to_id["car"])
print("word_to_id['happy']:", word_to_id["happy"])
print("word_to_id['lexus']:", word_to_id["lexus"])

```

### 2.4.5 PTB 데이터셋 평가

> sklearn의 고속 SVD를 이용한 통계 기반 기법 적용 코드

```python
# 코드 주소
# https://github.com/WegraLee/deep-learning-from-scratch-2/blob/master/ch02/count_method_big.py

import sys

sys.path.append("..")
import numpy as np
from common.util import most_similar, create_co_matrix, ppmi
from dataset import ptb

window_size = 2
wordvec_size = 100

corpus, word_to_id, id_to_word = ptb.load_data("train")
vocab_size = len(word_to_id)
print("동시발생 수 계산 ...")
C = create_co_matrix(corpus, vocab_size, window_size)
print("PPMI 계산 ...")
W = ppmi(C, verbose=True)

print("calculating SVD ...")
try:
    # truncated SVD (빠르다!)
    from sklearn.utils.extmath import randomized_svd

    U, S, V = randomized_svd(
        W, n_components=wordvec_size, n_iter=5, random_state=None
    )
    
except ImportError:
    # SVD (느리다)
    U, S, V = np.linalg.svd(W)

word_vecs = U[:, :wordvec_size]

querys = ["you", "year", "car", "toyota"]
for query in querys:
    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)
  
"""
결과
[query] you
 i: 0.6765483021736145
 we: 0.6494819521903992
 anybody: 0.5566297173500061
 do: 0.5514757037162781
 somebody: 0.5295705795288086

[query] year
 month: 0.6924706697463989
 quarter: 0.6354876756668091
 earlier: 0.5921391248703003
 last: 0.5782610177993774
 february: 0.5673093199729919

[query] car
 auto: 0.5763123631477356
 luxury: 0.5720716714859009
 corsica: 0.5537901520729065
 cars: 0.5228762030601501
 truck: 0.5090216994285583

[query] toyota
 motor: 0.7632917165756226
 mazda: 0.6594348549842834
 lexus: 0.6508239507675171
 motors: 0.6433226466178894
 nissan: 0.6395183205604553
"""
```

결과를 보면 

`you`는 `i`와 `we`와 높은 연관성을 가지고 있다.

`year`는 `month`와 `quarter`와 높은 연관성을 가지고 있다.

다른 단어들도 실제로 의미 혹은 문법적인 관점에서 연관성이 높은 단어들이 가까운 벡터로 나타난것을 확인할 수 있다.

