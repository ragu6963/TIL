``

# 4ì¥ word2vec ì†ë„ ê°œì„  [github](https://github.com/WegraLee/deep-learning-from-scratch-2)

ğŸ‘ì•ì¥ì˜ `word2vec`ì— ë‘ ê°€ì§€ ê°œì„ ì„ ì¶”ê°€í•´ì„œ ì†ë„ë¥¼ ê°œì„ í•´ë³´ì

1. `Embedding` ì´ë¼ëŠ” ê³„ì¸µì„ ë„ì…í•œë‹¤.
2. `ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§`ì´ë¼ëŠ” ìƒˆë¡œìš´ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ë„ì…í•œë‹¤.

## 4.1 word2vec ê°œì„  1

`CBOW`ëŠ” ê±°ëŒ€í•œ ë§ë­‰ì¹˜ë¥¼ ë‹¤ë£¨ê²Œ ë˜ë©´ ëª‡ ê°€ì§€ ë¬¸ì œê°€ ë°œìƒí•œë‹¤.

<img src="assets/4ì¥ word2vec ì†ë„ ê°œì„ /fig 4-2.png">

ìœ„ ê·¸ë¦¼ì—ì„œëŠ” ì…ë ¥ì¸µê³¼ ì¶œë ¥ì¸µì— ê° 100ë§Œ ê°œì˜ ë‰´ëŸ°ì´ ì¡´ì¬í•œë‹¤. ë§ì€ ë‰´ëŸ° ë•Œë¬¸ì— ì¤‘ê°„ ê³„ì‚°ì— `ë§ì€ ì‹œê°„`ì´ ì†Œìš”ëœë‹¤.

ì •í™•íˆëŠ” ë‹¤ìŒì˜ ë‘ ê³„ì‚°ì´ ë³‘ëª©ì´ ëœë‹¤.

1. ì…ë ¥ì¸µì˜ ì›í•« í‘œí˜„ê³¼ ê°€ì¤‘ì¹˜ í–‰ë ¬ $W~in$  ì˜ ê³± ê³„ì‚°
2. ì€ë‹‰ì¸µê³¼ ê°€ì¤‘ì¹˜ í–‰ë ¬ $W~out$ì˜ ê³± ë° Softmax ê³„ì¸µì˜ ê³„ì‚°

ì²« ë²ˆì§¸ëŠ” ì…ë ¥ì¸µì˜ ì›í•« í‘œí˜„ì˜ ë¬¸ì œì´ë‹¤. 

ì–´íœ˜ê°€ 100ë§Œ ê°œë¼ë©´ í•œ ì–´íœ˜ì˜ ì›í•« í‘œí˜„ì˜ ì›ì†Œ ìˆ˜ê°€ 100ë§Œ ê°œì˜ ë²¡í„°ê°€ ëœë‹¤. ìƒë‹¹í•œ ë©”ëª¨ë¦¬ë¥¼ ì°¨ì§€í•˜ê³ , ì´ ì›í•« ë²¡í„°ì™€ $W~in$ì„ ê³±í•˜ë©´ `ê³„ì‚° ìì›ì„ ìƒë‹¹íˆ ì†Œëª¨`í•œë‹¤.

ë‘ ë²ˆì§¸ëŠ” ì€ë‹‰ì¸µ ì´í›„ì˜ ê³„ì‚°ì´ë‹¤.

ë§ˆì°¬ê°€ì§€ë¡œ ì€ë‹‰ì¸µê³¼ $W~out$ì˜ ê³±ì˜ ê³„ì‚°ëŸ‰ì´ ì•„ì£¼ ë§ê³ , Softmax ê³„ì¸µì—ì„œë„ ê³„ì‚°ëŸ‰ì´ ì¦ê°€í•œë‹¤.

### 4.1.1 Embedding ê³„ì¸µ

ì• ì¥ì˜ word2vec ì—ì„œëŠ” `ë‹¨ì–´ì˜ ì›í•« í‘œí˜„`ê³¼ `ê°€ì¤‘ì¹˜ í–‰ë ¬`ê³¼ ê³±í–ˆë‹¤.

ë§Œì•½ ì–´íœ˜ ìˆ˜ê°€ 100ë§Œê°œ ì€ë‹‰ì¸µ ë‰´ëŸ°ì´ 100ê°œë¼ë©´ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì€ í–‰ë ¬ê³±ì´ ë°œìƒí•œë‹¤.

<img src="assets/4ì¥ word2vec ì†ë„ ê°œì„ /fig 4-3.png">

ê·¸ëŸ°ë° `í–‰ë ¬ê³±`ì´ í•˜ëŠ” ì¼ì€ ë‹¨ì§€ `ê°€ì¤‘ì¹˜ í–‰ë ¬`ì—ì„œ `íŠ¹ì • í–‰`ì„ ì¶”ì¶œí•  ë¿ì´ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ `í–‰ë ¬ê³±`ì€ ì‚¬ì‹¤ í•„ìš”ê°€ ì—†ë‹¤.

ì´ì²˜ëŸ¼ `ë‹¨ì–´ IDì— í•´ë‹¹í•˜ëŠ” íŠ¹ì • í–‰ ì¶”ì¶œ`ì„ í•˜ëŠ” ê³„ì¸µì„ `Embedding ê³„ì¸µ`ì´ë¼ê³  ë¶€ë¥¸ë‹¤.

### 4.1.2 Embedding ê³„ì¸µ êµ¬í˜„ 

```python
class Embedding:
    def __init__(self, W):
        self.params = [W]
        self.grads = [np.zeros_like(W)]
        self.idx = None

    def forward(self, idx):
        # ê°€ì¤‘ì¹˜ í–‰ë ¬
        W, = self.params
        
        # ë‹¨ì–´ id
        self.idx = idx
        
        # ê°€ì¤‘ì¹˜ í–‰ë ¬ì—ì„œ ë‹¨ì–´ idì— í•´ë‹¹í•˜ëŠ” í–‰ ì¶”ì¶œ
        out = W[idx]
        return out

    def backward(self, dout):
        # ê°€ì¤‘ì¹˜ ê¸°ìš¸ê¸° í–‰ë ¬
        dW, = self.grads
        
        # ì „ì²´ ì›ì†Œ ê°’ 0ìœ¼ë¡œ
        dW[...] = 0
        
        # ê°€ì¤‘ì¹˜ ê¸°ìš¸ê¸° í–‰ë ¬ì˜ íŠ¹ì • í–‰(ë‹¨ì–´ id)ì— ê¸°ìš¸ê¸° doutì„ í• ë‹¹
        np.add.at(dW, self.idx, dout)
        return None
```

<img src="assets/4ì¥ word2vec ì†ë„ ê°œì„ /fig 4-4.png">

#### ìˆœì „íŒŒ

`ê°€ì¤‘ì¹˜ í–‰ë ¬(W)`ì—ì„œ  `ë‹¨ì–´ ID(idx)`ì— í•´ë‹¹í•˜ëŠ” íŠ¹ì • í–‰ì„ ì¶”ì¶œí•œ í›„, ë‹¤ìŒ ì¸µìœ¼ë¡œ ì „ë‹¬í•œë‹¤.

#### ì—­ì „íŒŒ

`ê°€ì¤‘ì¹˜ ê¸°ìš¸ê¸° í–‰ë ¬(dW)` ì˜ ì›ì†Œë¥¼ ëª¨ë‘ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ê³ , `íŠ¹ì • í–‰(idx)`ì—  ì´ì „ ì¸µì—ì„œ ì˜¨ `ê¸°ìš¸ê¸°(dout)`ì„ `ë”í•´ì¤€ë‹¤`.

> ë”í•´ì£¼ëŠ” ì´ìœ ëŠ” ì¤‘ë³µ idxê°€ ìˆì„ ë•Œ ë¨¼ì € ë‚˜ì˜¨ idx í–‰ì˜ ê°’ì„ ë®ì–´ì“°ê²Œ ë˜ê³ , ê¸°ìš¸ê¸° ì†Œì‹¤ì´ ë°œìƒí•œë‹¤.

<img src="assets/4ì¥ word2vec ì†ë„ ê°œì„ /fig 4-5.png">

## 4.2 word2vec ê°œì„  2

`ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§`ì„ ì´ìš©í•´ì„œ `ì€ë‹‰ì¸µ ì´í›„ ì²˜ë¦¬`ì˜ ë³‘ëª©ì„ í•´ê²°í•˜ì.

### 4.2.1 ì€ë‹‰ì¸µ ì´í›„ ê³„ì‚°ì˜ ë¬¸ì œì 

<img src="assets/4ì¥ word2vec ì†ë„ ê°œì„ /fig 4-6.png">

`ì€ë‹‰ì¸µ ì´í›„`ì—ì„œ ê³„ì‚°ì´ ì˜¤ë˜ ê±¸ë¦¬ëŠ” ê³³ì€ ë‘ ê³³ì´ë‹¤.

1. ì€ë‹‰ì¸µì˜ ë‰´ëŸ°ê³¼ ê°€ì¤‘ì¹˜ í–‰ë ¬($W~out~$) ì˜ ê³±
2. Softmax ê³„ì¸µì˜ ê³„ì‚°

ì²« ë²ˆì§¸ëŠ” ê±°ëŒ€í•œ í–‰ë ¬ì„ ê³±í•˜ëŠ” ë¬¸ì œì´ë‹¤. ìœ„ ê·¸ë¦¼ì—ì„œëŠ” ì€ë‹‰ì¸µì˜ ë‰´ëŸ° ë²¡í„° í¬ê¸°ëŠ”`100`ì´ê³ , ê°€ì¤‘ì¹˜ í–‰ë ¬ì˜ í¬ê¸°ëŠ” `100 X 100ë§Œ`ì´ë‹¤. ì´ ë‘ í–‰ë ¬ì„ ê³±í•˜ê¸° ìœ„í•´ì„œëŠ” ë§ì€ ì‹œê°„ì´ ì†Œëª¨ëœë‹¤.

ë‘ ë²ˆì§¸ëŠ” Softmax ê³„ì¸µì€ ì–´íœ˜ê°€ ë§ì•„ì§ˆìˆ˜ë¡ ê³„ì‚°ëŸ‰ì´ ë§ì•„ì§„ë‹¤ëŠ” ì ì´ë‹¤.

<img src="assets/4ì¥ word2vec ì†ë„ ê°œì„ /e 4-1.png">

ìœ„ ì‹ì€ kë²ˆì§¸ ë‹¨ì–´ë¥¼ íƒ€ê¹ƒìœ¼ë¡œ í–ˆì„ ë•Œì˜ Softmax ê³„ì‚°ì´ë‹¤.

ë¶„ëª¨ ê°’ì„ ì–»ìœ¼ë ¤ë©´ exp ê³„ì‚°ì„ `100ë§Œ`ë²ˆ ìˆ˜í–‰í•´ì•¼í•œë‹¤.

### 4.2.2 ë‹¤ì¤‘ ë¶„ë¥˜ì—ì„œ ì´ì§„ ë¶„ë¥˜ë¡œ

ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” `ì´ì§„ ë¶„ë¥˜`ì— ìˆë‹¤.

âœ…ì •í™•í•˜ê²ŒëŠ” `ë‹¤ì¤‘ ë¶„ë¥˜`ë¥¼ `ì´ì§„ ë¶„ë¥˜`ë¡œ ê·¼ì‚¬í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•œ í¬ì¸íŠ¸ì´ë‹¤.

ì§€ê¸ˆê¹Œì§€ëŠ” `100ë§Œ ê°œì˜ ë‹¨ì–´ ì¤‘`ì—ì„œ `í•˜ë‚˜`ì˜ ì˜³ì€ ë‹¨ì–´ë¥¼ ì„ íƒí•˜ëŠ” ë¬¸ì œì˜€ë‹¤.

ì´ ë¬¸ì œë¥¼ `íƒ€ê¹ƒ ë‹¨ì–´ê°€ X ì¸ê°€?`ì— ëŒ€í•œ ì§ˆë¬¸ì— `Yes/No`ë¡œ ëŒ€ë‹µí•  ìˆ˜ ìˆëŠ” ì‹ ê²½ë§ì„ ìƒê°í•´ì•¼í•œë‹¤.

<img src="assets/4ì¥ word2vec ì†ë„ ê°œì„ /fig 4-7.png">

`ì´ì§„ ë¶„ë¥˜`ë¡œ ì ‘ê·¼í•˜ë©´ ì‹ ê²½ë§ ê³„ì¸µì„ ìœ„ ê·¸ë¦¼ì²˜ëŸ¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.

ì€ë‹‰ì¸µê³¼ ì¶œë ¥ ì¸µì˜ $W~out~$ì˜ ë‚´ì ì€

âœ… `í•˜ë‚˜ì˜ íŠ¹ì • ë‹¨ì–´ ë²¡í„°`ë§Œ ì¶”ì¶œí•˜ê³ ,  ê·¸ ë²¡í„°ì™€ ì€ë‹‰ì¸µ ë‰´ëŸ°ì˜ ë‚´ì ì„ ê³„ì‚°í•˜ë©´ ëœë‹¤.

> ê·¸ë¦¼ì—ì„œì˜ íŠ¹ì • ë‹¨ì–´ëŠ”  `say`

<img src="assets/4ì¥ word2vec ì†ë„ ê°œì„ /fig 4-8.png">

ìœ„ ê·¸ë¦¼ì²˜ëŸ¼ $W~out~$ì—ëŠ” ê° ë‹¨ì–´ IDì˜ ë‹¨ì–´ ë²¡í„°ê°€ ê°ê°ì˜ ì—´ë¡œ ì €ì¥ë˜ì–´ ìˆë‹¤.

`say`ì— ëŒ€í•œ ë‹¨ì–´ ë²¡í„°ë¥¼ ì¶”ì¶œí•´ì„œ ì€ë‹‰ì¸µ ë‰´ëŸ°ê³¼ì˜ ë‚´ì ì„ êµ¬í•˜ë©´ `say`ì— ëŒ€í•œ ìµœì¢… ì ìˆ˜ì¸ ê²ƒ ì´ë‹¤.

### 4.2.3 ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì™€ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨

`ì´ì§„ ë¶„ë¥˜`ë¥¼ ì‹ ê²½ë§ìœ¼ë¡œ í•´ê²°í• ë ¤ë©´ `ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜`ë¥¼ ì ìš©í•´ í™•ë¥ ë¡œ ë³€í™˜í•˜ê³ , `êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨`ë¥¼ ì†ì‹¤ í•¨ìˆ˜ë¡œ ì‚¬ìš©í•œë‹¤.

> ì‹œê·¸ëª¨ì´ë“œ ê³µì‹ê³¼ ê³„ì¸µ, ê·¸ë˜í”„

<img src="assets/4ì¥ word2vec ì†ë„ ê°œì„ /e 4-2.png">

<img src="assets/4ì¥ word2vec ì†ë„ ê°œì„ /fig 4-9.png">

âœ…`ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜`ë¥¼ ì ìš©í•´ `í™•ë¥  y`ë¥¼ ì–»ìœ¼ë©´, ì´ `êµì°¨ ì—íŠ¸ë¡œí”¼ ì˜¤ì°¨`ë¥¼ ì‚¬ìš©í•´ í™•ë¥  yë¡œë¶€í„° ì†ì‹¤ì„ êµ¬í•œë‹¤.

> êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ ê³µì‹
>
> y : ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ ì¶œë ¥, t : ì •ë‹µ ë ˆì´ë¸”(1 ë˜ëŠ” 0)

<img src="assets/4ì¥ word2vec ì†ë„ ê°œì„ /e 4-3-1616505942043.png">

<img src="assets/4ì¥ word2vec ì†ë„ ê°œì„ /fig 4-10-1616505959526.png">



ìœ„ ê·¸ë¦¼ì—ì„œ ì£¼ëª©í•  ê²ƒì€ `ì—­ì „íŒŒì˜ y-t`ì´ë‹¤.

ë§Œì•½, ì •ë‹µ ë ˆì´ë¸”`t`ì´ 1ì´ë¼ë©´, í™•ë¥ `y`ê°€ 1ì— ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ ì˜¤ì°¨ê°€ ì¤„ì–´ë“ ë‹¤ëŠ” ëœ»ì´ë‹¤.

ë°˜ëŒ€ë¡œ í™•ë¥ `y`ê°€ 1ë¡œë¶€í„° ë©€ì–´ì§€ë©´ ì˜¤ì°¨ê°€ ì»¤ì§„ë‹¤.

ì˜¤ì°¨ê°€ `ì‘ë‹¤ë©´ ì‘ê²Œ` í•™ìŠµí•˜ê³ , `í¬ë‹¤ë©´ í¬ê²Œ` í•™ìŠµí•  ê²ƒì´ë‹¤.

### 4.2.4 ë‹¤ì¤‘ ë¶„ë¥˜ì—ì„œ ì´ì§„ ë¶„ë¥˜ë¡œ êµ¬í˜„

> ë‹¤ì¤‘ ë¶„ë¥˜ë§ê³¼ ì´ì§„ ë¶„ë¥˜ë§

```python
# https://github.com/WegraLee/deep-learning-from-scratch-2/blob/master/ch04/negative_sampling_layer.py
import numpy as np
import collections 
    
    
class Embedding:
    def __init__(self, W):
        self.params = [W]
        self.grads = [np.zeros_like(W)]
        self.idx = None

    def forward(self, idx):
        W, = self.params
        self.idx = idx
        # ê°€ì¤‘ì¹˜ í–‰ë ¬ì—ì„œ ë‹¨ì–´ idì— í•´ë‹¹í•˜ëŠ” í–‰ ì¶”ì¶œ
        out = W[idx]
        return out

    def backward(self, dout):
        dW, = self.grads
        dW[...] = 0 
        np.add.at(dW, self.idx, dout)
        return None
    

class SigmoidWithLoss:
    def __init__(self):
        self.params, self.grads = [], []
        self.loss = None
        self.y = None  # sigmoidì˜ ì¶œë ¥
        self.t = None  # ì •ë‹µ ë°ì´í„°

    def forward(self, x, t):
        self.t = t
        
        # ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ ê²°ê³¼
        self.y = 1 / (1 + np.exp(-x))
		
        # í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨
        self.loss = cross_entropy_error(np.c_[1 - self.y, self.y], self.t)

        return self.loss

    def backward(self, dout=1):
        batch_size = self.t.shape[0]

        dx = (self.y - self.t) * dout / batch_size
        return dx
    
    
class EmbeddingDot:
    def __init__(self, W):
        # ì„ë² ë””ë“œ ê³„ì¸µ
        self.embed = Embedding(W)
        # ê°€ì¤‘ì¹˜
        self.params = self.embed.params
        # ê¸°ìš¸ê¸°
        self.grads = self.embed.grads
        # ìˆœì „íŒŒ ê³„ì‚° ê²°ê³¼ ì„ì‹œ ì €ì¥ ë³€ìˆ˜
        self.cache = None
	
    # h : ì€ë‹‰ì¸µ ë‰´ëŸ°, idx : íƒ€ê²Ÿ ë‹¨ì–´ID ë°°ì—´(ë¯¸ë‹ˆë°°ì¹˜)
    def forward(self, h, idx):
        # íƒ€ê²Ÿ ë‹¨ì–´
        target_W = self.embed.forward(idx)
        # ë‚´ì  ê³„ì‚° ë° í–‰ í•©ê³„ ê²Œì‚°
        out = np.sum(target_W * h, axis=1)
        
		# ê³„ì‚° ê²°ê³¼ ì„ì‹œ ì €ì¥
        self.cache = (h, target_W)
        return out
    

    def backward(self, dout):
        h, target_W = self.cache
        dout = dout.reshape(dout.shape[0], 1)

        dtarget_W = dout * h
        self.embed.backward(dtarget_W)
        dh = dout * target_W
        return dh

```

<img src="assets/4ì¥ word2vec ì†ë„ ê°œì„ /fig 4-14.png">

### 4.2.5 ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§

ì§€ê¸ˆê¹Œì§€ëŠ” ê¸ì •ì ì¸ ì˜ˆ(ì •ë‹µ)ì— ëŒ€í•´ì„œë§Œ í•™ìŠµí–ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ë¶€ì •ì ì¸ ì˜ˆ(ì˜¤ë‹µ)ì„ ì…ë ¥í•˜ë©´ ì–´ë–¤ ê²°ê³¼ê°€ ë‚˜ì˜¬ê¹Œ?

<img src="assets/4ì¥ word2vec ì†ë„ ê°œì„ /fig 4-16.png">

ìœ„ ê·¸ë¦¼ì²˜ëŸ¼ ë¶€ì •ì ì¸ ì˜ˆë¥¼ ì…ë ¥í–ˆì„ ë•Œ ì¶œë ¥ì´ `0`ì— ê°€ê¹ê²Œ í•´ì£¼ëŠ” `ê°€ì¤‘ì¹˜`ê°€ í•„ìš”í•˜ë‹¤.

âœ…í•˜ì§€ë§Œ ëª¨ë“  ë¶€ì •ì ì¸ ì˜ˆë¥¼ ëŒ€ìƒìœ¼ë¡œ ì´ì§„ ë¶„ë¥˜ë¥¼ í•™ìŠµì‹œí‚¬ ìˆ˜ëŠ” ì—†ë‹¤. ì´ì§„ë¶„ë¥˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ì˜ë¯¸ê°€ ì‚¬ë¼ì§€ê¸° ë•Œë¬¸ì´ë‹¤.

ê·¸ë˜ì„œ ê·¼ì‚¬ì ì¸ í•´ë²•ìœ¼ë¡œ ë¶€ì •ì  ì˜ˆë¥¼ ëª‡ ê°œë§Œ ì„ íƒí•œë‹¤. ì´ê²ƒì´ ë°”ë¡œ `ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§`ê¸°ë²•ì˜ ì˜ë¯¸ì´ë‹¤.

âœ…`ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§`ì€ ê¸ì •ì  ì˜ˆì— ëŒ€í•œ ì†ì‹¤ì„ êµ¬í•˜ê³ , ëª‡ ê°œì˜ ë¶€ì •ì  ì˜ˆì— ëŒ€í•œ ì†ì‹¤ì„ êµ¬í•´ì„œ ë”í•œ ê°’ì„ ìµœì¢… ì†ì‹¤`ê¸ì ì  ì˜ˆì˜ ì†ì‹¤ + ë¶€ì ì  ì˜ˆì— ëŒ€í•œ ì†ì‹¤ í•©`ë¡œ ì •í•œë‹¤.

 <img src="assets/4ì¥ word2vec ì†ë„ ê°œì„ /fig 4-17.png" style="zoom:50%;" >

ê¸ì ì  ì˜ˆ`say`ì™€ ë¶€ì •ì  ì˜ˆ `hello, I`ì˜ ì†ì‹¤ì„ ëª¨ë‘ ë”í•´ì„œ ìµœì¢…ì†ì‹¤ë¡œ ì‚¬ìš©í•˜ê³  ìˆë‹¤.

### 4.2.6 ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ì˜ ìƒ˜í”Œë§ ê¸°ë²•

âœ…`ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§`ì—ì„œ ë¶€ì •ì  ì˜ˆë¥¼ ìƒ˜í”Œë§í•˜ëŠ” ì¢‹ì€ ë°©ë²•ì€ ë§ë­‰ì¹˜ì˜ í†µê³„ ë°ì´í„°ë¥¼ ê¸°ì´ˆë¡œ ìƒ˜í”Œë§ í•˜ëŠ” ê²ƒì´ë‹¤.  

ë§ë­‰ì¹˜ì—ì„œ ë“±ì¥ ë¹ˆë„ê°€ ë†’ì€ ë‹¨ì–´ë¥¼ ë§ì´ ì¶”ì¶œí•˜ê³ , ë°˜ëŒ€ì˜ ê²½ìš°ì—ëŠ” ì ê²Œ ì¶”ì¶œí•˜ëŠ” ê²ƒì´ë‹¤.

<img src="assets/4ì¥ word2vec ì†ë„ ê°œì„ /fig 4-18.png">

> ìƒ˜í”Œë§ êµ¬í˜„ ì˜ˆ

<img src="assets/4ì¥ word2vec ì†ë„ ê°œì„ /image-20210323223828065.png">

> ìƒ˜í”Œë§ ê³µì‹, ê¸°ë³¸ í™•ë¥ ë¶„í¬ì— 0.75 ë¥¼ ì œê³±í•œë‹¤

<img src="assets/4ì¥ word2vec ì†ë„ ê°œì„ /e 4-4.png">

ìœ„ ì‹ì²˜ëŸ¼ 0.75 ë¥¼ ì œê³±í•˜ëŠ” ì´ìœ ëŠ” `í™•ë¥ ì´ ë‚®ì€ ë‹¨ì–´`ì˜ í™•ë¥ ì„ ì‚´ì© ë†’íˆê¸° ìœ„í•´ì„œì´ë‹¤.

 <img src="assets/4ì¥ word2vec ì†ë„ ê°œì„ /image-20210323224124028.png">

ìœ„ ì˜ˆì—ì„œ ë³´ì´ëŠ” ê²ƒì²˜ëŸ¼ 0.75 ë¥¼ ì œê³±í•¨ìœ¼ë¡œì¨ í™•ë¥ ì´ `ë‚®ì€ ë‹¨ì–´ëŠ” ë†’ì•„ì¡Œê³ `, `ë†’ì€ ë‹¨ì–´ëŠ” ë‚®ì•„ì¡Œë‹¤.`

### 4.2.7 ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ êµ¬í˜„

```python
# https://github.com/WegraLee/deep-learning-from-scratch-2/blob/master/ch04/negative_sampling_layer.py
from common.np import *  # import numpy as np
from common.layers import Embedding, SigmoidWithLoss
import collections


class EmbeddingDot:
    def __init__(self, W):
        self.embed = Embedding(W)
        self.params = self.embed.params
        self.grads = self.embed.grads
        self.cache = None

    def forward(self, h, idx):
        target_W = self.embed.forward(idx)
        out = np.sum(target_W * h, axis=1)

        self.cache = (h, target_W)
        return out

    def backward(self, dout):
        h, target_W = self.cache
        dout = dout.reshape(dout.shape[0], 1)

        dtarget_W = dout * h
        self.embed.backward(dtarget_W)
        dh = dout * target_W
        return dh


class UnigramSampler:
    def __init__(self, corpus, power, sample_size):
        self.sample_size = sample_size
        self.vocab_size = None
        self.word_p = None

        counts = collections.Counter()
        # ë‹¨ì–´ ë“±ì¥ ë¹ˆë„ìˆ˜ ê³„ì‚°
        for word_id in corpus:
            counts[word_id] += 1

        vocab_size = len(counts)
        self.vocab_size = vocab_size

        
        # ë“±ì¥ í™•ë¥  ê³„ì‚°
        self.word_p = np.zeros(vocab_size) # ë“±ì¥ í™•ë¥  ì €ì¥ ë³€ìˆ˜
        for i in range(vocab_size):
            self.word_p[i] = counts[i] 
            
		
        self.word_p = np.power(self.word_p, power) # ë¶„ì ê³„ì‚°
        self.word_p /= np.sum(self.word_p) # ë¶„ëª¨ ê³„ì‚°
	
    # target idë¥¼ ì œì™¸í•˜ê³ , ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ì„ í•œë‹¤.
    def get_negative_sample(self, target):
        batch_size = target.shape[0]

        if not GPU:
            negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)

            for i in range(batch_size):
                p = self.word_p.copy()
                target_idx = target[i]
                # targetì˜ í™•ë¥ ì„ 0ìœ¼ë¡œ í•œë‹¤.
                p[target_idx] = 0
                p /= p.sum()
                negative_sample[i, :] = np.random.choice(self.vocab_size, size=self.sample_size, replace=False, p=p)
        else:
            # GPU(cupyï¼‰ë¡œ ê³„ì‚°í•  ë•ŒëŠ” ì†ë„ë¥¼ ìš°ì„ í•œë‹¤.
            # ë¶€ì •ì  ì˜ˆì— íƒ€ê¹ƒì´ í¬í•¨ë  ìˆ˜ ìˆë‹¤.
            negative_sample = np.random.choice(self.vocab_size, size=(batch_size, self.sample_size),
                                               replace=True, p=self.word_p)

        return negative_sample


class NegativeSamplingLoss:
    def __init__(self, W, corpus, power=0.75, sample_size=5):
        # ìƒ˜í”Œë§ ì‚¬ì´ì¦ˆ
        self.sample_size = sample_size
        # ìƒ˜í”Œë§
        self.sampler = UnigramSampler(corpus, power, sample_size)
        # ì¶œë ¥í•¨ìˆ˜ ë° ì†ì‹¤í•¨ìˆ˜ ì €ì¥ ë¦¬ìŠ¤íŠ¸
        # self.loss_layers[0] -> ê¸ì •ì  ì˜ˆ(íƒ€ê¹ƒ)
        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]
        # ì„ë² ë””ë“œ ê³„ì¸µ ì €ì¥ ë¦¬ìŠ¤íŠ¸
        # self.embed_dot_layers[0] -> ê¸ì •ì  ì˜ˆ(íƒ€ê¹ƒ)
        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]

        self.params, self.grads = [], []
        for layer in self.embed_dot_layers:
            self.params += layer.params
            self.grads += layer.grads

    def forward(self, h, target):
        batch_size = target.shape[0]
        # ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ ë‹¨ì–´
        negative_sample = self.sampler.get_negative_sample(target)

        # ê¸ì •ì  ì˜ˆ ìˆœì „íŒŒ
        # ì„ë² ë””ë“œ ê³„ì¸µ ìˆœì „íŒŒ -> ì ìˆ˜ ì¶œë ¥
        score = self.embed_dot_layers[0].forward(h, target)
        # ê¸ì •ì  ì˜ˆ ì •ë‹µ ë ˆì´ë¸” -> 1
        correct_label = np.ones(batch_size, dtype=np.int32)
        # ì†ì‹¤í•¨ìˆ˜
        loss = self.loss_layers[0].forward(score, correct_label)

        # ë¶€ì •ì  ì˜ˆ ìˆœì „íŒŒ
        # ë¶€ì •ì  ì˜ˆ ì •ë‹µ ë ˆì´ë¸” -> 0
        negative_label = np.zeros(batch_size, dtype=np.int32)
        for i in range(self.sample_size):
            negative_target = negative_sample[:, i]
            # ì„ë² ë””ë“œ ê³„ì¸µ ìˆœì „íŒŒ -> ì ìˆ˜ ì¶œë ¥
            score = self.embed_dot_layers[1 + i].forward(h, negative_target)
            loss += self.loss_layers[1 + i].forward(score, negative_label)

        return loss

    def backward(self, dout=1):
        dh = 0
        # ì—­ì „íŒŒ ê³„ì¸µì„ ë°˜ëŒ€ë¡œ ìˆ˜í–‰
        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):
            dscore = l0.backward(dout)
            dh += l1.backward(dscore)

        return dh
```

## 4.3 ê°œì„ íŒ word2vec í•™ìŠµ

 ### 4.3.1 CBOW ëª¨ë¸ êµ¬í˜„

```python
# https://github.com/WegraLee/deep-learning-from-scratch-2/blob/master/ch04/cbow.py
import numpy as np
from common.layers import Embedding
from ch04.negative_sampling_layer import NegativeSamplingLoss


class CBOW:
    # ì–´íœ˜ìˆ˜, ì€ë‹‰ì¸µ ë‰´ëŸ° ìˆ˜, ìœˆë„ìš° ì‚¬ì´ì¦ˆ, ë‹¨ì–´ ID ëª©ë¡
    def __init__(self, vocab_size, hidden_size, window_size, corpus):
        V, H = vocab_size, hidden_size

        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        W_in = 0.01 * np.random.randn(V, H).astype('f')
        W_out = 0.01 * np.random.randn(V, H).astype('f')

        # ê³„ì¸µ ìƒì„±
        self.in_layers = []
        for i in range(2 * window_size):
            layer = Embedding(W_in)  # Embedding ê³„ì¸µ ì‚¬ìš©
            self.in_layers.append(layer) # ê³„ì¸µ ëª¨ìœ¼ê¸°
            
       	# ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ ê³„ì¸µ ì‚¬ìš©
        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)

        # ëª¨ë“  ê°€ì¤‘ì¹˜ì™€ ê¸°ìš¸ê¸°ë¥¼ ë°°ì—´ì— ëª¨ì€ë‹¤.
        layers = self.in_layers + [self.ns_loss]
        self.params, self.grads = [], []
        for layer in layers:
            self.params += layer.params
            self.grads += layer.grads

        # ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ì— ë‹¨ì–´ì˜ ë¶„ì‚° í‘œí˜„ì„ ì €ì¥í•œë‹¤.
        self.word_vecs = W_in
	
    def forward(self, contexts, target):
        h = 0
        for i, layer in enumerate(self.in_layers):
            h += layer.forward(contexts[:, i])
        h *= 1 / len(self.in_layers)
        loss = self.ns_loss.forward(h, target)
        return loss

    def backward(self, dout=1):
        dout = self.ns_loss.backward(dout)
        dout *= 1 / len(self.in_layers)
        for layer in self.in_layers:
            layer.backward(dout)
        return None
```

