# 8장 어텐션 [github](https://github.com/WegraLee/deep-learning-from-scratch-2)

## 8.1 어텐션의 구조

**`어텐션 메커니즘`**덕분에 seq2seq가 필요한 정보에만 주목할 수 있게 된다.

### 8.1.1 seq2seq의 문제점

Encoder의 출력은 **`고정 길이 벡터`**는 큰 문제가 있다.

✅입력 문장의 길이에 상관없이 항상 같은 길이의 벡터로 변환하게되면 긴 문장의 경우 모든 정보를 벡터에 담을 수 없게된다.

<img src='assets/8장 어텐션/fig 8-1.png'>

### 8.1.2 Encoder 개선

✅ 문제점을 개선하기 위해 **`시각별(단어별)`**로 은닉 상태 벡터를 저장해서 Encoder의 출력 길이를 입력 문장 길이와 동일하게 하는 것이 좋다. 

여기서 각 은닉 상태에는 바로 전에 입력된 단어에 대한 정보가 많이 포함되어 있을 것이다. 

아래 그림에서 "고양이" 벡터에는 "나", "는", "고양이" 에 대한 정보가 담겨있지만 특히 "고양이"에 대한 정보가 많이 담겨있을 것이다.

<img src='assets/8장 어텐션/fig 8-2.png'>

### 8.1.3 Decoder 개선 1

앞장까지는 마지막 은닉 상태 벡터만 Decoder에서 사용하였다.

<img src='assets/8장 어텐션/fig 8-5.png'>

✅ 시각별 은닉 상태를 사용하기 위해서는 새로운 계산 계층이 추가된다. 

이 계산 계층은 (Encoder의 ) 시각별 은닉 상태 벡터와 (Decoder 의) 각 시각별 LSTM의 출력을 입력을 받고,  Affine 계층으로 출력한다.

<img src='assets/8장 어텐션/fig 8-6.png'>

✅ 위 신경망의 목적은 **`얼라이먼트(단어의 대응관계를 나타내는 정보)`** 추출이다.

즉, Decoder가 I 출력할 때, hs에서 "나"에 대응하는 벡터를 선택하겠다는 의미이다. 
그리고 이 선택 작업은 어떤 계산 계층에서 이루어진다.

그런데 선택하는 작업은 미분할 수 없다는 문제가 있다. 

✅ 그래서 `모든 것을 선택`해서 `가중치`를 계산해서 원하는 벡터를 구한다.

각 단어의 중요도는 `가중치(a)`로 나타내고, hs 와 a의 가중치합을 구하여 원하는 벡터(가중치 합)를 얻는다.

<img src='assets/8장 어텐션/fig 8-7.png'>

<img src='assets/8장 어텐션/fig 8-8.png'>

> 가중치 합 계산 구현

```python
import numpy as np

T, H = 5, 4
hs = np.random.randn(T, H)
a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])

# 그림 8-9
# (1,5)행렬을 (5,1)로 변환하고, (5,1) 행렬을 4개 복사해서 (5,4) 행렬을 만든다.
ar = a.reshape(5,1).repeat(4, axis=1)
print(ar.shape) # (5, 4)

# 가중치 곱
t = hs * ar
print(t.shape) # (5, 4)

# 가중치 합
c = np.sum(t, axis=0) 
print(c.shape) # (1,4)
```

<img src='assets/8장 어텐션/fig 8-9.png'>

> 미니배치 처리용 가중치 합

```python
import numpy as np

N, T, H = 10, 5, 4
hs = np.random.randn(N, T, H)
a = np.random.randn(N, T)
ar = a.reshape(N, T, 1).repeat(H, axis=2) 

t = hs * ar
print(t.shape) # (10, 5, 4)

c = np.sum(t, axis=1) 
print(c.shape) # (10, 1, 4)
```

<img src='assets/8장 어텐션/fig 8-11.png'>

✅ `Repeate` 노드로 a를 복제하고, hs와 원소별 곱을 계산한 후 `Sum` 노드로 가중치 합을 구한다.

> 계층 구현

```python
class WeightSum:
    def __init__(self):
        self.params, self.grads = [], []
        self.cache = None

    def forward(self, hs, a):
        N, T, H = hs.shape

        ar = a.reshape(N, T, 1)
        # repeate 대신 넘파이의 브로드캐스트 이용
        t = hs * ar
        c = np.sum(t, axis=1)

        self.cache = (hs, ar)
        return c

    def backward(self, dc):
        hs, ar = self.cache
        N, T, H = hs.shape
        dt = dc.reshape(N, 1, H).repeat(T, axis=1)
        dar = dt * hs
        dhs = dt * ar
        da = np.sum(dar, axis=2)

        return dhs, da
```

### 8.1.4 Decoder 개선 2

✅ 가중치 **`a`**를 구하기 위해서는 내적과 소프트 맥스를 사용하면 된다.

<img src='assets/8장 어텐션/fig 8-12.png'>

Decoder의 LSTM 계층의 은닉 상태 벡터 `h` 와 `hs`의 각 단어 벡터의 유사도는 `내적`으로 계산한다.

> 내적은 두 벡터가 얼마나 같은 방향을 향하고 있는가를 의미한다. 그러므로 유사도를 나타내기 좋은 방법이다. 

<img src='assets/8장 어텐션/fig 8-13.png'>

<img src='assets/8장 어텐션/fig 8-14.png'>

두 벡터의 내적의 결과는 `s` 이고, 이 s 는 정규화하기 전의 값이다. 정규화 하기 위해 `Softmax` 함수를 적용한다.

<img src='assets/8장 어텐션/fig 8-15.png'>

> 계층 구현 

```python
# https://github.com/WegraLee/deep-learning-from-scratch-2/blob/master/ch08/attention_layer.py
import sys
sys.path.append('..')
import numpy as np
from common.layers import Softmax


class AttentionWeight:
    def __init__(self):
        self.params, self.grads = [], []
        self.softmax = Softmax()
        self.cache = None

    def forward(self, hs, h):
        N, T, H = hs.shape

        hr = h.reshape(N, 1, H)
        t = hs * hr 
        s = np.sum(t, axis=2)
        a = self.softmax.forward(s)

        self.cache = (hs, hr)
        return a

    def backward(self, da):
        hs, hr = self.cache
        N, T, H = hs.shape

        ds = self.softmax.backward(da)
        dt = ds.reshape(N, T, 1).repeat(H, axis=2)
        dhs = dt * hr
        dhr = dt * hs
        dh = np.sum(dhr, axis=1)

        return dhs, dh
```

### 8.1.5 Decoder 개선 3

✅ **`Weight Sum 과 Attention Weight`** 계층을 하나로 결합해보자

<img src='assets/8장 어텐션/fig 8-16.png'>

<img src='assets/8장 어텐션/fig 8-17.png'>

```python
# https://github.com/WegraLee/deep-learning-from-scratch-2/blob/master/ch08/attention_layer.py

class Attention:
    def __init__(self):
        self.params, self.grads = [], []
        # AttentionWeight 계층과 WeightSum 계층
        self.attention_weight_layer = AttentionWeight()
        self.weight_sum_layer = WeightSum()
        # 가중치 a
        self.attention_weight = None

    def forward(self, hs, h):
        # 가중치 a 계산
        a = self.attention_weight_layer.forward(hs, h)
        # 가중치 합 계산
        out = self.weight_sum_layer.forward(hs, a)
        # 가중치 a 저장
        self.attention_weight = a
        return out

    def backward(self, dout):
        dhs0, da = self.weight_sum_layer.backward(dout)
        dhs1, dh = self.attention_weight_layer.backward(da)
        dhs = dhs0 + dhs1
        return dhs, dh
```

### 8.2 어텐션을 갖춘 seq2seq 구현

### 8.2.1 Encoder 구현

✅ 이번 장의 Encoder 클래스의 forward() 메서드는 모든 은닉 상태를 반환한다.

```python
# https://github.com/WegraLee/deep-learning-from-scratch-2/blob/master/ch08/attention_seq2seq.py
import sys
sys.path.append('..')
from common.time_layers import *
from ch07.seq2seq import Encoder, Seq2seq
from ch08.attention_layer import TimeAttention


class AttentionEncoder(Encoder):
    def forward(self, xs):
        xs = self.embed.forward(xs)
        hs = self.lstm.forward(xs)
        return hs
    
    """
    이전 장 Encoder 순전파
    def forward(self, xs): 
        xs = self.embed.forward(xs)
        hs = self.lstm.forward(xs)
        self.hs = hs
        return hs[:, -1, :]
    """

    def backward(self, dhs):
        dout = self.lstm.backward(dhs)
        dout = self.embed.backward(dout)
        return dout

```

### 8.2.2 Decoder 구현

✅ Softmax 계층 이전까지의 Decoder를 구현해보자. 순전파와 역전파 그리고 새로운 단어열을 생성하는 메서드를 추가한다.

```python
# https://github.com/WegraLee/deep-learning-from-scratch-2/blob/master/ch08/attention_seq2seq.py
import sys
sys.path.append('..')
from common.time_layers import *
from ch07.seq2seq import Encoder, Seq2seq
from ch08.attention_layer import TimeAttention

class AttentionDecoder:
    def __init__(self, vocab_size, wordvec_size, hidden_size):
        V, D, H = vocab_size, wordvec_size, hidden_size
        rn = np.random.randn

        embed_W = (rn(V, D) / 100).astype('f')
        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')
        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')
        lstm_b = np.zeros(4 * H).astype('f')
        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')
        affine_b = np.zeros(V).astype('f')

        self.embed = TimeEmbedding(embed_W)
        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)
        # Attention 계층
        self.attention = TimeAttention()
        self.affine = TimeAffine(affine_W, affine_b)
        layers = [self.embed, self.lstm, self.attention, self.affine]

        self.params, self.grads = [], []
        for layer in layers:
            self.params += layer.params
            self.grads += layer.grads

    def forward(self, xs, enc_hs):
        h = enc_hs[:,-1]
        self.lstm.set_state(h)

        out = self.embed.forward(xs)
        dec_hs = self.lstm.forward(out)
        c = self.attention.forward(enc_hs, dec_hs)
        out = np.concatenate((c, dec_hs), axis=2)
        score = self.affine.forward(out)

        return score

    def backward(self, dscore):
        dout = self.affine.backward(dscore)
        N, T, H2 = dout.shape
        H = H2 // 2

        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]
        denc_hs, ddec_hs1 = self.attention.backward(dc)
        ddec_hs = ddec_hs0 + ddec_hs1
        dout = self.lstm.backward(ddec_hs)
        dh = self.lstm.dh
        denc_hs[:, -1] += dh
        self.embed.backward(dout)

        return denc_hs

    def generate(self, enc_hs, start_id, sample_size):
        sampled = []
        sample_id = start_id
        h = enc_hs[:, -1]
        self.lstm.set_state(h)

        for _ in range(sample_size):
            x = np.array([sample_id]).reshape((1, 1))

            out = self.embed.forward(x)
            dec_hs = self.lstm.forward(out)
            c = self.attention.forward(enc_hs, dec_hs)
            out = np.concatenate((c, dec_hs), axis=2)
            score = self.affine.forward(out)

            sample_id = np.argmax(score.flatten())
            sampled.append(sample_id)

        return sampled
```

### 8.2.3 seq2seq 구현

```python
# https://github.com/WegraLee/deep-learning-from-scratch-2/blob/master/ch08/attention_seq2seq.py

class AttentionSeq2seq(Seq2seq):
    def __init__(self, vocab_size, wordvec_size, hidden_size):
        args = vocab_size, wordvec_size, hidden_size
        # Attention 인코더와 디코더
        self.encoder = AttentionEncoder(*args)
        self.decoder = AttentionDecoder(*args)
        # Softmax 계층
        self.softmax = TimeSoftmaxWithLoss()

        self.params = self.encoder.params + self.decoder.params
        self.grads = self.encoder.grads + self.decoder.grads
```

## 8.3 어텐션 평가

✅ 날짜형식을 변경하는 문제로 **`어텐션 seq2seq`**을 평가해보자.

### 8.3.1 날짜 형식 변환 문제

아래 그림과 같은 변환을 하는 것이 목표이다.

날짜 형식 변환을 목표로 한 것은 두 가지 이유가 있다.

첫 번째는 보기보다 어려운 문제이기 때문이다. 입력되는 날짜 데이터에는 다양한 형식이 존재할 수 있고, 사람이 변환 규칙을 수작업으로 쓰기에는 매우 귀찮은 일이다.

두 번째는 입력과 출력 사이의 대응 관계가 쉽기 때문이다.`년 월 일` 

<img src='assets/8장 어텐션/fig 8-22.png'>

### 8.3.2 어텐션을 갖춘 seq2seq 학습

```python
# coding: utf-8
import sys
sys.path.append('..')
sys.path.append('../ch07')
import numpy as np
import matplotlib.pyplot as plt
from dataset import sequence
from common.optimizer import Adam
from common.trainer import Trainer
from common.util import eval_seq2seq
from attention_seq2seq import AttentionSeq2seq
from ch07.seq2seq import Seq2seq
from ch07.peeky_seq2seq import PeekySeq2seq


# 데이터 읽기
(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')
char_to_id, id_to_char = sequence.get_vocab()

# 입력 문장 반전
x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]

# 하이퍼파라미터 설정
vocab_size = len(char_to_id)
wordvec_size = 16
hidden_size = 256
batch_size = 128
max_epoch = 10
max_grad = 5.0

# 앞 장과 달리 Attention이 적용된 seq2seq 모델을 사용한다.
model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)
# model = Seq2seq(vocab_size, wordvec_size, hidden_size)
# model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)

optimizer = Adam()
trainer = Trainer(model, optimizer)

acc_list = []
for epoch in range(max_epoch):
    trainer.fit(x_train, t_train, max_epoch=1,
                batch_size=batch_size, max_grad=max_grad)

    correct_num = 0
    for i in range(len(x_test)):
        question, correct = x_test[[i]], t_test[[i]]
        verbose = i < 10
        correct_num += eval_seq2seq(model, question, correct,
                                    id_to_char, verbose, is_reverse=True)

    acc = float(correct_num) / len(x_test)
    acc_list.append(acc)
    print('정확도 %.3f%%' % (acc * 100))


model.save_params()

# 그래프 그리기
x = np.arange(len(acc_list))
plt.plot(x, acc_list, marker='o')
plt.xlabel('에폭')
plt.ylabel('정확도')
plt.ylim(-0.05, 1.05)
plt.show()
```

<img src='assets/8장 어텐션/fig 8-26.png'>

아무것도 적용하지 않은 seq2seq에 비해 많은 성능향상을 보여주고 있다.

또한, 엿보기를 적용한 모델에 비해 빠른 학습속도를 보여주고 이싿.

### 8.3.3 어텐션 시각화

✅ 어텐션이 시계열 변환을 수행할 때, 어느 원소에 높은 가중치를 주는지 알 수 있다.

구현에서 Time Attention 계층의 attention_weights에 각 시각의 어텐션 가중치가 저장된다.

```python
# coding: utf-8
import sys
sys.path.append('..')
import numpy as np
from dataset import sequence
import matplotlib.pyplot as plt
from attention_seq2seq import AttentionSeq2seq


(x_train, t_train), (x_test, t_test) = \
    sequence.load_data('date.txt')
char_to_id, id_to_char = sequence.get_vocab()

# 입력 문장 반전
x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]

vocab_size = len(char_to_id)
wordvec_size = 16
hidden_size = 256

model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)
# 가중치 파일 불러오기
model.load_params()

_idx = 0
# 시각화 함수
def visualize(attention_map, row_labels, column_labels):
    fig, ax = plt.subplots()
    ax.pcolor(attention_map, cmap=plt.cm.Greys_r, vmin=0.0, vmax=1.0)

    ax.patch.set_facecolor('black')
    ax.set_yticks(np.arange(attention_map.shape[0])+0.5, minor=False)
    ax.set_xticks(np.arange(attention_map.shape[1])+0.5, minor=False)
    ax.invert_yaxis()
    ax.set_xticklabels(row_labels, minor=False)
    ax.set_yticklabels(column_labels, minor=False)

    global _idx
    _idx += 1
    plt.show()


np.random.seed(1984)
for _ in range(5):
    idx = [np.random.randint(0, len(x_test))]
    x = x_test[idx]
    t = t_test[idx]

    model.forward(x, t)
    d = model.decoder.attention.attention_weights
    d = np.array(d)
    attention_map = d.reshape(d.shape[0], d.shape[2])

    # 출력하기 위해 반전
    attention_map = attention_map[:,::-1]
    x = x[:,::-1]

    row_labels = [id_to_char[i] for i in x[0]]
    column_labels = [id_to_char[i] for i in t[0]]
    column_labels = column_labels[1:]

    visualize(attention_map, row_labels, column_labels)
```

년도는 년도끼리 월은 월끼리 일은 일끼리 높은 가중치를 보여주고 있다.

<img src='assets/8장 어텐션/fig 8-27.png'>

## 8.4 어텐션에 관한 남은 이야기

### 8.4.1 양방향 RNN

 만약 `나는 고양이로소 이다` 라는 문장을 Encoder에 입력했을 때 `고양이`에 대응하는 벡터에는 `나, 는, 고양이`에 대한 정보가 인코딩되어 저장될 것이다. 

<img src='assets/8장 어텐션/fig 8-29.png'>

✅ 그런데 앞의 정보만 아니라 주변 정보를 균등하게 담고 싶다면 **`양방향 LSTM(양방향 RNN)`**을 이용할 수 있다.

아래 그림처럼 양방향 LSTM은 역방향으로 처리하는 LSTM 계층이 추가된 것이다. 

<img src='assets/8장 어텐션/fig 8-30.png'>

### 8.4.2 Attention 계층 사용 방법

> 일반적인 Attention 계층 사용과  Attention 계층의 출력을 다음 LSTM 계층에 입력시키는 방법

<img src='assets/8장 어텐션/fig 8-31-1619316440471.png'>

✅ 아래 그림에서는 Attention 계층의 출력을 다음 시각의 LSTM 계층에 입력했다.

이렇게하면 LSTM 계층이 Decoder의 맥락 벡터 정보를 이용할 수 있다.

다만, 이런 방법이 정확도에 어떤 영향을 줄지는 실제로 해보지 않으면 모른다.

<img src='assets/8장 어텐션/fig 8-32.png'>

### 8.4.3 seq2seq 심층화와 skip 연결

LSTM 계층을 깊게 쌓기 위해 아래처럼 계층을 구성할 수 있다.

Encoder와 Decder는 같은 층수의 LSTM을 사용하고, Decoder의 첫 번째 LSTM의 출력을 Attention 계층에 입력한 후 그 출력을 Decoder의 다른 계층으로 전파한다.

<img src='assets/8장 어텐션/fig 8-33.png'>

✅ 층을 깊게 할 때 사용되는 중요한 기법 중 **`skip 연결`** 이라는게 있다.

skip 연결은 계층을 넘어서 선을 연결하는 방법이다. 아래 그림처럼 계층을 건너뛰고 연결하는 것이다.

skip 연결에서는 덧셈 노드가 핵심이다. 왜냐하면 덧셈 노드는 역전파 시 기울기를 그대로 흘려보내기 때문에 기울기에 영향을 주지 않기 떄문이다.

<img src='assets/8장 어텐션/fig 8-34.png'>

## 8.5 어텐션 응용

### 8.5.1 구글 신경망 기계 번역(GNMT)

기계 번역은 규칙 기반 번역, 용례 기반 번역, 통계 기반 번역 그리고 현재 `신경망 기계 번역`으로 흘러왔다.

GNMT도 Encoder와 Decoder, Attention으로 구성된다. 또한, LSTM의 깊은 계층, 양방향 LSTM, skip 등 다양한 개선도 더해졌다

<img src='assets/8장 어텐션/fig 8-35.png'>

GNMT는 기존 기법에 비해 높은 정확도를 보여주고 있고, 사람에 가까워 지고있다.

<img src='assets/8장 어텐션/fig 8-36.png'>

### 8.5.2 트랜스포머

✅ RNN은 시간 방향으로 병렬 계산할 수 없다는 단점이 있다.(GPU 환경을 사용할 수 없다)

RNN을 없애는 연구 중 가장 유명한 것은 **``트랜스포머``** 모델이다.

트랜스포머는 어텐션으로 구성되는데, 그중 **`셀프어텐션`** 이라는 기술이 핵심이다.

✅Self-Attention은 하나의 시계열 데이터를 대상으로 한 어텐션으로, `하나의 시계열 데이터 내에서`각 원소가 다른 원소들과 어떻게 관련되는지 살펴보자는 취지이다.

트랜스포머는 Encoder와 Decoder에서 모두 RNN 대신 Attention을 사용한다. 

셀프어텐션은 RNN과 달리 `하나의 시계열`에서 처리가 가능하므로 GPU를 이용해 병렬 계산이 가능하다.

또한, 번역 품질도 상당하게 높일 수 있었다.

<img src='assets/8장 어텐션/fig 8-37.png'>

<img src='assets/8장 어텐션/fig 8-38.png'>

<img src='assets/8장 어텐션/fig 8-39.png'>

### 8.5.3 뉴럴 튜링 머신(NTM)

✅ 뉴럴 튜링 머신은 외부 메모리 사용에 대한 연구이다.

아래 그림의 한가운데에 있는 것은 `컨트롤러(신경막 혹은 RNN)` 라는 모듈이다.

이 컨트롤러는 0 혹은 1 데이터를 처리하여 새로운 데이터를 출력한다.

여기서 중요한 것은 컨트롤러의 오른쪽위에 있는 `큰 종이(메모리)`이다.

이 메모리 덕분에 컨트롤러는 컴퓨터와 같은 능력을 얻는다. 바로 큰 종이에 필요한 정보를 쓰거나 불필요한 정보를 지우는 능력, 필요한 정보를 다시 읽어 들이는 능력이다.

이처럼 NTM은 외부 메모리를 읽고 쓰면서 시계열 데이터를 처리한다. 또한, 메모리 조작을 `미분 가능`한 계산으로 구축했다.

따라서 메모리 조작 순서도 데이터로부터 학습할 수 있다.



<img src='assets/8장 어텐션/fig 8-40.png'>

아래 그림은 간략화한 NTM 계층 구성이다.

LSTM은 컨트롤러가 되어 NTM의 주된 처리를 수행한다.

Write Head는 LSTM 계층의 은닉 상태를 받아서 필요한 정보를 메모리에 저장한다.

Read Head는 메모리로부터 중요한 정보를 읽어서 다음 시각의 LSTM 계층에 전달한다.

✅ 컴퓨터의 메모리 조작을 모방하기 위해서 2개의 어텐션을 이용한다. `콘텐츠 기반 어텐션`과 `위치 기반 어텐션`이다.

콘텐츠 기반 어텐션은 지금까지 본 어텐션과 동일하고, 입력으로 주어진 벡터와 비슷한 벡터를 메모리에서 찾는 역할을 한다.

위치 기반 어텐션은 이전 시각에서 주목한 메모리의 위치를 기준으로 그 전후로 이동하는 용도이다.

실제로 NTM은 seq2seq만으로는 풀리지 않던 복잡한 문제에서도 높은 성과를 보여주고있다. 

<img src='assets/8장 어텐션/fig 8-41.png'>