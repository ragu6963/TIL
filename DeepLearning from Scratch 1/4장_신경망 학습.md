# 4장 신경망 학습

> `신경망`의 특징은 데이터를 보고 학습할 수 있다는 점이다.
> 그 말은 즉, 데이터를 보고 자동으로 가중치를 결정한다는 뜻이다.

### 4.1.1 데이터 주도 학습

`기계학습`은 데이터가 생명이다. 데이터가 없으면 아무것도 시작할 수 없다.

`기계학습`에서는 데이터에서 규칙을 찾아내는 역할을 `기계`가 담당하지만 이미지에서 벡터로 변환할 때 사용하는 특징은 여전히 `사람`이 설계해야한다.

반면 `신경망`은 아래의 그림처럼 사람이 개입하지 않는다.

> 기계학습과 신경망(딥러닝)

<img src="4장_신경망 학습.assets/fig 4-2.png">

> 이처럼 신경망`딥러닝`은 이미지에 포함된 중요한 특징까지 `기계`가 스스로 학습한다.



### 4.1.2 훈련 데이터와 시험데이터

기계학습에서는 데이터를 훈련 데이터`training data`와 시험 데이터`test data`로 나눠 수행하는 것이 일반적이다.

1. 우선 훈련 데이터만 사용하여 최적의 매개변수`가중치, 편향`를 찾는다.
2. 시험 데이터를 사용하여 모델의 성능을 평가한다.

이렇게 데이터를 나누는 이유는 `범용`적인 모델을 얻기 위해서이다.` 범용능력`은 아직 보지 못한 데이터도 올바르게 문제를 풀어내는 능력이다.

`범용능력` 을 얻는 것이 기계학습의 최종 목표이다.

> 참고로 한 데이터셋에만 지나치게 최적화된 상태를 오버피팅`overfittinng` 이라고 한다.

---

## 4.2 손실 함수

`손실함수`란 신경망의 성능지표이다. 일반적으로는 `오차제곱합`과 `교차 엔트로피 오차`를 사용한다.

> 손실함수는 신경망 성능의 나쁨을 나타내는 지표이다. 

### 4.2.1 오차제곱합

<img src="4장_신경망 학습.assets/e 4-1.png">

```python
# E = 1/2 * ∑ _k (yk-tk)²
# yk : 신경망의 출력
# tk : 정답 레이블
# k : 데이터의 차원 수
# 오차 제곱합 코드
def sum_squared_error(y, t):
    return 0.5 * np.sum((y - t) ** 2)

# 정답은 '2'
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]

# ex1 '2'일 확률이 가장 높다고 추정함(0.6)
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
sse = sum_squared_error(np.array(y), np.array(t))
print(sse)  # 0.0975
# ex2 '7'일 확률이 가장 높다고 추정함(0.6)
y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
sse = sum_squared_error(np.array(y), np.array(t))
print(sse)  # 0.5975

# 첫 번째의 손실함수가 작고, 정답 레이블과의 오차도 작은 것을 알 수 있다.
# 즉, 오차제곱합 기준으로 더 작을수록 정답에 가깝다는 뜻이다.
```



`y_k`는 신경망의 출력, `t_k`는 정답 레이블, `k`는 데이터의 차원수

`오차제곱합` = sum((각 원소의 추정값 - 실제값) ** 2) / 2

 ### 4.2.2 교차 엔트로피 오차

<img src="4장_신경망 학습.assets/e 4-2.png">





`log`는 자연로그 `y_k`는 출력 `t_k`는 정답이다. 여기서 `t`는 정답 인덱스만 1이고 나머지는 0이다. 실질적으로 정답일 때의 자연로그를 계산하는 식이다.

예를들어 정답이 '2'가 정답이고, 출력이 0.6 이면 교차 엔트로피 오차는 `-log0.6 = 0.51` 이다.

```python
# E = -∑ _k (tk * log(yk))
# log : 자연로그
# yk : 신경망의 출력
# tk : 정답 레이블(one-hot encoding)
# k : 데이터의 차원 수
# 실질적으로 정답일때의 추정의 자연로그를 계산하는 식이 됨
def cross_entropy_error(y, t):
    delta = 1e-7  # 0일때 -무한대가 되지 않기 위해 작은 값을 더함
    return -np.sum(t * np.log(y + delta))

t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
cee = cross_entropy_error(np.array(y), np.array(t)
print(cee)  # 0.510825457099
y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
cee = cross_entropy_error(np.array(y), np.array(t))
print(cee)  # 2.30258409299

# 정답인 첫번째의 예에서는 교차 엔트로피는 출력이 0.6이고, 오차는 약 0.51이다
# 오답인 두번째는 출력이 0.1이고, 오차는 2.3이다.
# 교차엔트로피 오차도 오차 값이 작을수록 정답일 가능성이 높다는 것을 보여준다.
```

### 4.2.3 미니배치 학습

훈련 데이터로부터 일부만 골라 학습을 수행하는 것을`미니배치`라고 한다.

훈련 데이터가  수백만에서 수천만일 때 현실적으로 모든 데이터를 계산하는 것은 현실적이지 않기 때문에 일부만 계산해서 `근사치`로 이용한다.

### 4.2.5 손실 함수를 구하는 이유

`정확도`를 놔두고 `손실함수`를 구하는 이유는 미분`기울기` 때문이다.

신경망 학습에서는 최적의 매개변수를 탐색할 때 `손실함수`의 값을 `가능한 한 작게하는 `매개변수 값을 찾는다.

이 때 매개변수의 `기울기`를 계산하고, 그 값을 단서로 매개변수의 값을 서서히 갱신한다.

---

신경망이 있을 때 `가중치에 대한 손실 함수의 기울기`는 가중치를 아주 조금 변화 했을 때 `손실함수가 어떻게 변하냐`는 의미이다. 

만약 기울기 값이 음수면 가중치 매개변수를 양의 방향으로, 만약 기울기 값이 양수면 가중치 매개변수를 음의 방향으로 변화시켜 손실 함수의 값을 줄일 수 있다.

그러나 기울기가 0이라면 어느 쪽으로 움직여도 손실 함수의 값은 줄어들지 않고, 갱신은 거기서 끝이난다.

 그런데 `정확도`를 지표로 삼으면 대부분의 장소에서 `기울기` 값이 `0`이 되어 매개변수를 갱신할 수 없다.

---

`정확도`의 기울기가 대부분 0인 이유는 가중치를 아주 조금 변화시켰을 때 정확도가 바뀌지 않기 때문이다.

예를들어 100장의 훈련 데이터 중 32장`32%`을 올바르게 인식했을 때 가중치를 아주 조금 변화시켜도 똑같이 32장`32%`을 인식할 것 이다.

또한 정확도가 개선되더라도 연속적인 값이 아닌 33장`33%`,34장`34%` 와 같이 불연속적일 것이다.

---

마찬가지로 `계단함수` 를 쓰지않는 이유도 대부분의 구간에서 기울기가 0이기 때문이다.

> `선형함수`는 기울기가 일정하기 때문에 쓰지 않는 것인가??

## 4.3 수치 미분 

`경사법`에서는 기울기 값을 기준으로 나아갈 방향을 정한다.

### 4.3.1 미분

`미분`은 한순간의 변화량`기울기`의 표시이다.

> 미분 계산의 나쁜 구현과 좋은 구현

```python
# 나쁜 구현 예(전방차분)
def numerical_diff_bad(f, x):
    h = 10e-50 # 0의 값이 나온다.
    return (f(x + h) - f(x)) / h
# h값이 너무 작아 반올림 오차를 일으킬 수 있음 10e-4정도가 적당하다고 알려짐
# 전방 차분에서는 차분이 0이 될 수 없어 오차가 발생
# x+h 와 x 의 함수 f의 차분을 계산하지만 오차가 있다는 것에 주의해야한다.
```

실제 미분은 `x에서의 기울기`를 구하지만 위의 예시 코드에서는 `x+h와 x 사이의 기울기`를 구하게 된다.

그래서 실제 미분값과 위의 예의 미분값은 동일하지 않다.

> 나쁜 구현의 그래프

<img src="4장_신경망 학습.assets/fig 4-5.png">

```python
# 좋은 구현(중심 혹은 중앙 차분) 
def numerical_diff(f, x):
    h = 10e-4
    return (f(x + h) - f(x - h)) / (2 * h)
```

x+h와 x 사이의 오차를 줄이기 위해 (x+h)와 (x-h)의 차분을 계산한다.

이처럼 아주 작은 차분으로 미분하는 것을 수치미분이라고 한다. `수치미분`은 기울기를 근사치로 계산하는 방법이다.

### 4.3.3 편미분

변수가 여러개인 함수에 대한 미분을 `편미분`이라고 한다.

<img src="4장_신경망 학습.assets/e 4.6.png">

> 위 식에 대한 편미분 계산

```python
# x0 = 3, x1 = 4일 때, x0에 대한 편미분
def function_tmp1(x0):
    return x0 ** 2 + 4.0 ** 2.0
 

# x0 = 3, x1 = 4일 때, x1에 대한 편미분
def function_tmp2(x1):
    return 3.0 ** 2.0 + x1 * x1


print(numerical_diff(function_tmp1, 3.0))  # 5.999999999998451
print(numerical_diff(function_tmp2, 4.0))  # 8.000000000000895

# 변수가 하나인 함수를 정의하고, 그 함수를 미분하는 형태로 구현하였다.
# 예를들어 1에서는 x1 = 4로 고정하고, 2에서는 x0 = 3로 고정해서 풀었다.
```



## 4.4 기울기

`기울기`란 모든 변수의 `편미분`을 벡터로 정리한 것이다.

> 기울기 그림

<img src="4장_신경망 학습.assets/fig 4-9.png">

> 기울기가 가르키는 방향이 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향이다.

```python
import numpy as np
import matplotlib.pylab as plt


# 앞 절에서 x0, x1에 대한 편미분을 변수별로 따로 계산했음.
# x0, x1의 편미분을 동시에 계산하고 싶다면?
# x0 = 3, x1 = 4일 때 (x0, x1) 양쪽의 편미분을 묶어 벡터로 정리한 것을 기울기gradient라고 한다.
def numerical_gradient(f, x):
    h = 1e-4
    grad = np.zeros_like(x)  # x와 형상이 같은 배열을 생성

    for idx in range(x.size):
        # 원래 값 보관
        tmp_val = x[idx]
        # f(x+h) 계산
        x[idx] = tmp_val + h
        fxh1 = f(x)

        # f(x-h) 계산
        x[idx] = tmp_val - h
        fxh2 = f(x)

        # (f(x+h) - f(x-h)) / 2*h
        grad[idx] = (fxh1 - fxh2) / (2 * h)
        x[idx] = tmp_val  # 값 복원

    return grad
```



### 4.4.1 경사하강법

신경망은 손실 함수가 최솟값이 될 때의 매개변수를 학습시에 찾아야 한다. 

그러나 일반적인 문제의 손실 함수는 매우 복잡하다.그렇기 때문에 어떤 값이 최솟값인지 알 수 없다.

그래서 기울기를 이용해 손실 함수의 최솟값을 찾으려는 것이 `경사하강법`이다.

주의할점은 기울기가 가르키는 곳이 정말 최솟값이 있다고 보장할 수 없다는 것이다. 

> 실제로 복잡한 함수에서는 최솟값이 가르키는 방향에 최솟값이 없는 경우가 대부분이다.

<img src="4장_신경망 학습.assets/e 4.7.png">

`η` 는 갱신하는 양을 나타낸다. 신경망 학습에서는 학습률`learning rate`라고 한다. 매개변수 값을 얼마나 갱신하느냐를 정하는 것이 학습률이다.

일반적으로 이 값이 너무 크거나 작으면 `좋은 값`를 찾을 수 없다. 보통 학습률을 변경하면서 올바르게 학습하고 있는지 확인하면서 진행한다.

>  f(x0,x1) = x0^2 + x1^2의 그림과 풀이 코드

```python
# f : 최적화하려는 함수
# init_x : 초깃값
# lr : 학습률
# step_num : 반복횟수
def gradient_descent(f, init_x, lr=0.01, step_num=100):
    x = init_x
    x_history = []

    for i in range(step_num):
        # 가중치 변화 저장
        x_history.append(x.copy())
        grad = numerical_gradient(f, x)
        x -= lr * grad

    return x, np.array(x_history)

# f(x0, x1) = x0² + x1²
def function_2(x):
    return x[0] ** 2 + x[1] ** 2
    # or return np.sum(x**2)

# 초기값 : -3,4
init_x = np.array([-3.0, 4.0])
gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)

# lr 별 결과 값
# lr=0.1 ,  [-7.63888491e-10,  1.01851799e-09]
# lr=10.0, 	[-2.58983747e+13, -1.29524862e+12]
# lr=1e-10, [-2.99999994,  3.99999992]

```

> 결과 값에서 보이는 것처럼 학습률`lr`이 너무 크면 값이 발산하고, 너무 작으면 학습이 끝나지 않는다.

<img src="4장_신경망 학습.assets/fig 4-10.png">

### 4.4.2 신경망에서의 기울기

신경망에서의 기울기는 가중치에 대한 손실 함수의 기울기이다. 가중치가 `W` 손실함수가 `L` 일 때의 수식은 다음과 같다.

<img src="4장_신경망 학습.assets/e 4.8.png">

`∂L/∂W` 의 각 원소는 각 원소에 대한 편미분이다.  주의할점은 `W`와 `∂L/∂W`의 `shape`가 같다는 점이다.

> 간단한 신경망의 기울기 구하는 코드

```python
def cross_entropy_error(y, t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)

    if t.size == y.size:
        t = t.argmax(axis=1)

    batch_size = y.shape[0]
    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size


def softmax(x):
    if x.ndim == 2:
        x = x.T
        x = x - np.max(x, axis=0)
        y = np.exp(x) / np.sum(np.exp(x), axis=0)
        return y.T

    x = x - np.max(x)  # 오버플로 대책
    return np.exp(x) / np.sum(np.exp(x))


def numerical_gradient(f, x):
    h = 1e-4  # 0.0001
    grad = np.zeros_like(x)

    it = np.nditer(x, flags=["multi_index"], op_flags=["readwrite"])
    while not it.finished:
        idx = it.multi_index
        tmp_val = x[idx]
        x[idx] = tmp_val + h
        fxh1 = f(x)  # f(x+h)

        x[idx] = tmp_val - h
        fxh2 = f(x)  # f(x-h)
        grad[idx] = (fxh1 - fxh2) / (2 * h)

        x[idx] = tmp_val  # 값 복원
        it.iternext()

    return grad


import numpy as np


class simpleNet:
    # shape가 2 X 3인 랜덤 값 인스턴스 변수
    def __init__(self):
        self.W = np.random.randn(2, 3)  # 정규분포로 초기화

    # 예측 수행
    def predict(self, x):
        return np.dot(x, self.W)

    # 손실 함수 값 계산
    # x : 입력 데이터, t : 정답 레이블
    def loss(self, x, t):
        z = self.predict(x)
        y = softmax(z)
        loss = cross_entropy_error(y, t)

        return loss


net = simpleNet()
print(net.W)
print("-" * 64)
x = np.array([0.6, 0.9])
p = net.predict(x)
print(p)
print(np.argmax(p))  # 최댓값의 인덱스
print("-" * 64)
t = np.array([0, 0, 1])  # 정답 레이블
print(net.loss(x, t))
print("-" * 64)


def f(W):
    return net.loss(x, t)


dw = numerical_gradient(f, net.W)
print(dw)

# dw는 손실함수의 기울기이다.
# 값 예 : [[ 0.11509829  0.44991151 -0.5650098 ] [ 0.17264744  0.67486727 -0.84751471]]
# ∂L/∂W_11 은 대략 0.11인데 w_11을 h만큼 늘리면 손실함수는 0.11h 만큼 증가한다는 의미이다.
# ∂L/∂W_13 은 대략 -0.56인데 w_13을 h만큼 늘리면 손실함수는 0.56h 만큼 줄어든다는 의미이다.
# 그리고 W_13이 W_11 보다 크게 기여한다는 사실도 알 수 있다.
```



 ## 4.5 학습 알고리즘 구현하기

1. 미니배치 : 훈련 데이터 중 일부를 무작위로 가져온다.
2. 기울기 산출 : 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게하는 방법을 제시한다.
3. 매개변수 갱신 : 가중치,편향 매개변수를 기울기 방향으로 아주 조금 갱신한다.
4. 1~3 반복

> 데이터를 미니배치로 무작위로 선정하고, 경사 하강법으로 매개변수를 갱신하기 때문에 확률적 경사 하강법`SGD` 이라고 부른다.



### 4.5.1 2층 신경망 클래스 구현

> two_layer_net

```python
import sys, os
sys.path.append(os.pardir)   
from common.functions import *
from common.gradient import numerical_gradient
import numpy as np


class TwoLayerNet:
    def __init__(
        self, input_size, hidden_size, output_size, weight_init_std=0.01
    ):
        # 가중치 편향 초기화
        self.params = {}
        # 1번째의 가증치와 편향 W1, b1
        self.params["W1"] = weight_init_std * np.random.randn(
            input_size, hidden_size
        )
        self.params["b1"] = np.zeros(hidden_size)
        
        
        # 2번째의 가증치와 편향 W2, b2
        self.params["W2"] = weight_init_std * np.random.randn(
            hidden_size, output_size
        )
        self.params["b2"] = np.zeros(output_size)
	
	# 예측
    def predict(self, x):
        W1, W2 = self.params["W1"], self.params["W2"]
        b1, b2 = self.params["b1"], self.params["b2"]
		
        # 각 은닉층의 계산과 활성화 함수(sigmoid)
        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)
        a2 = np.dot(z1, W2) + b2
        y = softmax(a2)

        return y

    # x : 입력 데이터, t : 정답 레이블
    def loss(self, x, t):
        y = self.predict(x)

        return cross_entropy_error(y, t)
    
    # 정확도 계산
    # x : 입력 데이터, t : 정답 레이블
    def accuracy(self, x, t):
        # y : 입력 데이터로 예측한 값
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        # t : 정답
        t = np.argmax(t, axis=1)

        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy

    # x : 입력 데이터, t : 정답 레이블
    # 수치미분 방식으로 손실 함수에 대한 기울기 계산
    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)

        grads = {}
        # 가중치와 편향의 손실함수에 대한 기울기 계산
        grads["W1"] = numerical_gradient(loss_W, self.params["W1"])
        grads["b1"] = numerical_gradient(loss_W, self.params["b1"])
        grads["W2"] = numerical_gradient(loss_W, self.params["W2"])
        grads["b2"] = numerical_gradient(loss_W, self.params["b2"])

        return grads
	
    # numerical_gradient() 개선버전
    # 역전파 이용
    def gradient(self, x, t):
        W1, W2 = self.params["W1"], self.params["W2"]
        b1, b2 = self.params["b1"], self.params["b2"]
        grads = {}

        batch_num = x.shape[0]

        # forward
        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)
        a2 = np.dot(z1, W2) + b2
        y = softmax(a2)

        # backward
        dy = (y - t) / batch_num
        grads["W2"] = np.dot(z1.T, dy)
        grads["b2"] = np.sum(dy, axis=0)

        da1 = np.dot(dy, W2.T)
        dz1 = sigmoid_grad(a1) * da1
        grads["W1"] = np.dot(x.T, dz1)
        grads["b1"] = np.sum(dz1, axis=0)

        return grads
    
def numerical_gradient(f, x):
    h = 1e-4  # 0.0001
    grad = np.zeros_like(x)

    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])
    while not it.finished:
        idx = it.multi_index
        tmp_val = x[idx]
        x[idx] = tmp_val + h
        fxh1 = f(x)  # f(x+h)

        x[idx] = tmp_val - h
        fxh2 = f(x)  # f(x-h)
        grad[idx] = (fxh1 - fxh2) / (2*h)

        x[idx] = tmp_val  # 값 복원
        it.iternext()

    return grad
```

> train_neuralnet.py

```python
import sys, os
sys.path.append(os.pardir) 
import numpy as np
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist
from two_layer_net import TwoLayerNet
import time

# 데이터 읽기
(x_train, t_train), (x_test, t_test) = load_mnist(
    normalize=True, one_hot_label=True
)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

# 하이퍼파라미터
iters_num = 10000  # 반복 횟수를 적절히 설정한다.
train_size = x_train.shape[0]
batch_size = 100  # 미니배치 크기
learning_rate = 0.1

train_loss_list = []
train_acc_list = []
test_acc_list = []

# 1에폭당 반복 수
# train_size = 60000 , batch_size = 100
# iter_per_epoch = train_size / batch_size = 600
iter_per_epoch = max(train_size / batch_size, 1)

# 미니배치 -> 무작위
# 60000개를 나눠서 100 0~99 100~199

for i in range(iters_num):
    # 시간 측정
    # start = time.time()
    # print(i)
    
    # 미니배치 
    # batch_mask = 0 ~ train_size 에서 batch_size 만큼의 값 랜덤으로 추출
    # 예) train_size = 60000 , batch_size = 100
    # batch_mask = [1,5090 ,24 ,50, ...]
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]

    # 기울기 계산
    grad = network.numerical_gradient(x_batch, t_batch)  # 21초
    # grad = network.gradient(x_batch, t_batch)  # 0.001 / 17.7초

    # 매개변수 갱신
    for key in ("W1", "b1", "W2", "b2"):
        network.params[key] -= learning_rate * grad[key]

    # 학습 경과 기록
    # 손실함수 변화 저장
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)

    # 1에폭당 정확도 계산
    # 1에포크 = 600회
    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))
	
    # 시간 측정
    # end = time.time()
    # print(end - start)


# 그래프 그리기
markers = {"train": "o", "test": "s"}
x = np.arange(len(train_acc_list))
plt.plot(x, train_acc_list, label="train acc")
plt.plot(x, test_acc_list, label="test acc", linestyle="--")
plt.xlabel("epochs")
plt.ylabel("accuracy")
plt.ylim(0, 1.0)
plt.legend(loc="lower right")
plt.show()
```



