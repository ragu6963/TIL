> 도서 : 밑바닥 부터 시작하는 딥러닝 [링크](https://www.hanbit.co.kr/store/books/look.php?p_code=B8475831198) [깃허브](https://github.com/WegraLee/deep-learning-from-scratch)

# 6장 학습 관련 기술들

이번 장에서는 `매개변수 탐색 최적화 방법` `가중치 초깃값` `하이퍼파라미터 설정` `오버피팅 대응책인 가중치 감소` `드롭아웃등의 정규화 방법`을 다룬다.

## 6.1 매개변수 갱신

지금까지 사용한 `SGD`의 단점을 알아보고, 다른 최적화 기법을 알아보자.

### 6.1.2 확률적 경사 하강법(SGD)

> SGD 수식

<img src="6장_학습 관련 기술들.assets/e 6.1.png">

`W`는 갱신할 가중치, `미분값`은 `W`에 대한 `손실 함수`의 기울기, `에타`는 학습률이다.

SGD 는 기울어진 방향으로 일정 거리만 가겠다는 단순한 방법이다.

> 파이썬 코드 구현

```python
class SGD:
    def __init__(self, lr = 0.01):
        self.lr = lr
    def update(self, params, grads):
        for key in params.keys():
            params[key] -= self.lr * grads[key]
```

> 신경망 매개변수 진행 의사코드

```
network = TwoLayerNet(...)
optimizer = SGD()
for i in range(...):
    ...
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    grads = network.gradient(x_batch, t_batch)
    params = network.params
    optimizer.update(params, grads)
```

매개변수 갱신은 `optimizer ` 가 수행하고, 우리는 매개변수와 기울기 정보만 제공한다.

### 6.1.3 SGD의 단점

단점을 알아보기 위한 함수의 식을 보자

<img src="6장_학습 관련 기술들.assets/e 6.2-1613565357019.png">

> 위 식의 기울기

<img src="6장_학습 관련 기술들.assets/fig 6-2.png">

 식이 최솟값이 되는 좌표는 (x, y) = (0, 0)이지만, 그림에서의 기울기 대부분은 (0, 0) 을 가리키지 않는다.

초깃값을 (x, y) = (-7, 2)일 때 그림을 보자

<img src="6장_학습 관련 기술들.assets/fig 6-3.png">

`SGD`는 위 그림처럼 심하게 굽이진, 상당히 비효율적인 움직임을 보여준다.

`SGD의 단점`은 비등방성 함수`방향에 따라 성질, 여기서는 기울기가 달라지는 함수`에서는 비효율 적이다.

또한, 기울어진 방향이 본래의 최솟값과 다른 방향을 가리킬 수도 있다는 점이 있다.

### 6.1.4 모멘텀

`모멘텀`은 운동량을 뜻하는 수식은 아래처럼 쓸 수 있다.

<img src="6장_학습 관련 기술들.assets/e 6.3-1613565634075.png">

<img src="6장_학습 관련 기술들.assets/e 6.4.png">

새롭게 등장한 `v`는 속도를 뜻한다.

`모멘텀`은 기울기 방향으로 가속시키고, 힘을 받지 않을 때에는 서서히 하강시킨다. 

```python
class Momentum:
    def __init__(self, lr=0.01, momentum=0.9):
        self.lr = lr
        self.momentum = momentum
        self.v = None # 물체의 속도, 초기 = 0
        
    def update(self, params, grads):
        # 처음 호출시에는 매개변수와 같은 구조의 데이터(value = 0)를 딕셔너리 변수로 저장
        if self.v is None:
            self.v = {}
            for key, val in params.items():                                
                self.v[key] = np.zeros_like(val)
        
        # 두번째 부터 위 식 구현 코드
        for key in params.keys():
            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] 
            params[key] += self.v[key]
```

> 모맨텀의 기울기

<img src="6장_학습 관련 기술들.assets/fig 6-5.png">

`SGD`와 비교하면 지그재그 하는 정도가 덜 하다.

`x축`의 힘은 아주 작지만 방향은 변하지 않아서 한 방향으로 일정하게 가속하고,

`y축`의 힘은 크지만 위아래로 번갈아 받아서 상충하여 속도는 안정적이지 않다.

### 6.1.5 AdaGrad

학습률을 정하는 효과적 기술로 `학습률 감소 learning rate decay`가 있다. 이는 학습을 진행하면 서 `lr`을 점차 줄이는 방법이다.

`AdaGrad`는 개별 매개변수에 적응적으로 학습률을 조정하면서 학습한다.

> AdaGrad 공식

<img src="6장_학습 관련 기술들.assets/e 6.5.png">

<img src="6장_학습 관련 기술들.assets/e 6.6.png">

`h`는 기울기 값을 제곱하여 계속 더해준다. 그리고 매개변수를 갱신할 때 `1/(h ** 0.5)`학습률을 조정한다.

매개변수의 원소에서 크게 갱신된 원소는 학습률이 낮아진다는 뜻이다.

>AdaGrad 구현

```python
class AdaGrad:
    def __init__(self, lr=0.01):
        self.lr = lr
        self.h = None

    def update(self, params, grads):
        # 처음 값은 0
        if self.h is None:
            self.h = {}
            for key, val in params.items():
                self.h[key] = np.zeros_like(val)
		
        for key in params.keys():
            self.h[key] += grads[key] * grads[key]
            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)
            # 1e-7 는 h 가 0이라도 오류가 나는 것을 방지한다.
```

> AdaGrad 경로

<img src="6장_학습 관련 기술들.assets/fig 6-6.png">

`y축` 방향은 기울기가 커서 처음에는 크게 움직이지만, 큰 움직이에 반비례해서 학습률을 낮춘다.

### 6.1.6 Adam

`Adam`은 모멘텀과 AdaGrad를 융합하자는 생각으로 출발한 기법이다.

모멘텀과 AdaGrad의 융합뿐만 아니라 하이퍼파라미터의 편향 보정 또한 진행 된다.



### 갱신 방법 비교

> 기울기 갱신 과정 비교

<img src="6장_학습 관련 기술들.assets/fig 6-8.png">

> MNIST 데이터셋 비교

<img src="6장_학습 관련 기술들.assets/fig 6-9.png">