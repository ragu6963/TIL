> 도서 : 밑바닥 부터 시작하는 딥러닝 [링크](https://www.hanbit.co.kr/store/books/look.php?p_code=B8475831198) [깃허브](https://github.com/WegraLee/deep-learning-from-scratch)

# 6장 학습 관련 기술들

이번 장에서는 `매개변수 탐색 최적화 방법` `가중치 초깃값` `하이퍼파라미터 설정` `오버피팅 대응책인 가중치 감소` `드롭아웃등의 정규화 방법`을 다룬다.

## 6.1 매개변수 갱신

지금까지 사용한 `SGD`의 단점을 알아보고, 다른 최적화 기법을 알아보자.

### 6.1.2 확률적 경사 하강법(SGD)

> SGD 수식

<img src="6장_학습 관련 기술들.assets/e 6.1.png">

`W`는 갱신할 가중치, `미분값`은 `W`에 대한 `손실 함수`의 기울기, `에타`는 학습률이다.

SGD 는 기울어진 방향으로 일정 거리만 가겠다는 단순한 방법이다.

> 파이썬 코드 구현

```python
class SGD:
    def __init__(self, lr = 0.01):
        self.lr = lr
    def update(self, params, grads):
        for key in params.keys():
            params[key] -= self.lr * grads[key]
```

> 신경망 매개변수 진행 의사코드

```
network = TwoLayerNet(...)
optimizer = SGD()
for i in range(...):
    ...
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    grads = network.gradient(x_batch, t_batch)
    params = network.params
    optimizer.update(params, grads)
```

매개변수 갱신은 `optimizer ` 가 수행하고, 우리는 매개변수와 기울기 정보만 제공한다.

### 6.1.3 SGD의 단점

단점을 알아보기 위한 함수의 식을 보자

<img src="6장_학습 관련 기술들.assets/e 6.2-1613565357019.png">

> 위 식의 기울기

<img src="6장_학습 관련 기술들.assets/fig 6-2.png">

 식이 최솟값이 되는 좌표는 (x, y) = (0, 0)이지만, 그림에서의 기울기 대부분은 (0, 0) 을 가리키지 않는다.

초깃값을 (x, y) = (-7, 2)일 때 그림을 보자

<img src="6장_학습 관련 기술들.assets/fig 6-3.png">

`SGD`는 위 그림처럼 심하게 굽이진, 상당히 비효율적인 움직임을 보여준다.

`SGD의 단점`은 비등방성 함수`방향에 따라 성질, 여기서는 기울기가 달라지는 함수`에서는 비효율 적이다.

또한, 기울어진 방향이 본래의 최솟값과 다른 방향을 가리킬 수도 있다는 점이 있다.

### 6.1.4 모멘텀

`모멘텀`은 운동량을 뜻하는 수식은 아래처럼 쓸 수 있다.

<img src="6장_학습 관련 기술들.assets/e 6.3-1613565634075.png">

<img src="6장_학습 관련 기술들.assets/e 6.4.png">

새롭게 등장한 `v`는 속도를 뜻한다.

`모멘텀`은 기울기 방향으로 가속시키고, 힘을 받지 않을 때에는 서서히 하강시킨다. 

```python
class Momentum:
    def __init__(self, lr=0.01, momentum=0.9):
        self.lr = lr
        self.momentum = momentum
        self.v = None # 물체의 속도, 초기 = 0
        
    def update(self, params, grads):
        # 처음 호출시에는 매개변수와 같은 구조의 데이터(value = 0)를 딕셔너리 변수로 저장
        if self.v is None:
            self.v = {}
            for key, val in params.items():                                
                self.v[key] = np.zeros_like(val)
        
        # 두번째 부터 위 식 구현 코드
        for key in params.keys():
            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] 
            params[key] += self.v[key]
```

> 모맨텀의 기울기

<img src="6장_학습 관련 기술들.assets/fig 6-5.png">

`SGD`와 비교하면 지그재그 하는 정도가 덜 하다.

`x축`의 힘은 아주 작지만 방향은 변하지 않아서 한 방향으로 일정하게 가속하고,

`y축`의 힘은 크지만 위아래로 번갈아 받아서 상충하여 속도는 안정적이지 않다.

### 6.1.5 AdaGrad

학습률을 정하는 효과적 기술로 `학습률 감소 learning rate decay`가 있다. 이는 학습을 진행하면 서 `lr`을 점차 줄이는 방법이다.

`AdaGrad`는 개별 매개변수에 적응적으로 학습률을 조정하면서 학습한다.

> AdaGrad 공식

<img src="6장_학습 관련 기술들.assets/e 6.5.png">

<img src="6장_학습 관련 기술들.assets/e 6.6.png">

`h`는 기울기 값을 제곱하여 계속 더해준다. 그리고 매개변수를 갱신할 때 `1/(h ** 0.5)`학습률을 조정한다.

매개변수의 원소에서 크게 갱신된 원소는 학습률이 낮아진다는 뜻이다.

>AdaGrad 구현

```python
class AdaGrad:
    def __init__(self, lr=0.01):
        self.lr = lr
        self.h = None

    def update(self, params, grads):
        # 처음 값은 0
        if self.h is None:
            self.h = {}
            for key, val in params.items():
                self.h[key] = np.zeros_like(val)
		
        for key in params.keys():
            self.h[key] += grads[key] * grads[key]
            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)
            # 1e-7 는 h 가 0이라도 오류가 나는 것을 방지한다.
```

> AdaGrad 경로

<img src="6장_학습 관련 기술들.assets/fig 6-6.png">

`y축` 방향은 기울기가 커서 처음에는 크게 움직이지만, 큰 움직이에 반비례해서 학습률을 낮춘다.

### 6.1.6 Adam

`Adam`은 모멘텀과 AdaGrad를 융합하자는 생각으로 출발한 기법이다.

모멘텀과 AdaGrad의 융합뿐만 아니라 하이퍼파라미터의 편향 보정 또한 진행 된다.



### 갱신 방법 비교

> 기울기 갱신 과정 비교

<img src="6장_학습 관련 기술들.assets/fig 6-8.png">

> MNIST 데이터셋 비교

<img src="6장_학습 관련 기술들.assets/fig 6-9.png">

## 6.2 가중치의 초깃값

### 6.2.1 초깃값을 0으로 하면?

초깃값을 0으로 하면 학습이 이루어 지지 않는다.(정확히는 `균일한 값`으로 설정하면 안된다.)

왜냐하면 오차역전파법에서 모든 가중치의 값이 똑같이 갱신되기 때문이다. 

곱셈 노드의 역전파를 생각해보면 `두 값이 교차되서 곱해지는데 ` 모든 값이 같다면 학습을 진행해도 같은 값을 유지한다.

`가중치의 대칭적인 구조`를 무너뜨리기 위해 초깃값을 무작위로 설정한다.

### 6.2.2 은닉충의 활성화값 분포

가중치의 초기값에 따라 은닉층에서 활성화값의 변화를 보자.

> 가중치가 표준편차가 1인 정규분포 , 시그모이드 함수

<img src="6장_학습 관련 기술들.assets/fig 6-10.png">

활성화값들이 `0, 1 ` 에 모여있다. 

시그모이드 함수는 출력이 0 또는 1에 가까워질수록 기울기가 0에 가까워진다.

그래서 위의 그래프처럼 `0, 1`에 치우쳐지면 역전파의 기울기 값이 점점 작아지다가 사라진다.

이것이 `기울기 소실 gradient vanishing`라고 알려진 문제이다. 

> 가중치가 표준편차가 0.01인 정규분포 , 시그모이드 함수

<img src="6장_학습 관련 기술들.assets/fig 6-11.png">`0.5`에 모여있기 때문에 기울기 소실은 일어나지 않지만, 활성화값들이 치우쳤다는 것은 `표현력 관점`에서 큰 문제가 있다는 것이다.

> 다수의 뉴런이 거의 같은 출력하고 있으니 뉴런이 여러개인 의미가 없어 진다는 뜻이다.

그래서 활성화 값이 치우쳐지면 `표현력의 제한`한다는 관점에서 문제가 된다.

---

일반적인 딥러닝 프레임워크들의 표준인 `Xavier 초깃값`을 써보자.

> Xavier 초깃값 이론

<img src="6장_학습 관련 기술들.assets/fig 6-12.png">

`Xavier`를 사용하면 앞 층에 노드가 많을수록 대상 노드의 초깃값으로 설정하는 가중치가 좁게 퍼진다.

> 가중치가 Xavier 초깃값 , 시그모이드 함수

<img src="6장_학습 관련 기술들.assets/fig 6-13.png">

층이 깊어질수록 형태가 다소 일그러지지만, 앞의 두 방식보다는 넓게 분포된 것을 확인할 수 있다.

### 6.2.3 ReLU를 사용할 때의 가중치 초깃값

`Xavier 초깃값`은 활성화함수가  선형인 것을 전제로 한다.

비선형 함수인 `ReLU`에서는 `He 초깃값`이라는 방법을 사용한다.

`He 초깃값`은 노드가 n개일 때, 표준편차가`sqrt(2/n)` 인 정규분포를 사용한다.

<img src="6장_학습 관련 기술들.assets/fig 6-14.png">

`std = 0.01`일때, 활성화값들이 아주 작다. 신경망에 아주 작은 데이터가 흐른다는 것은 역전파 때 가중치의 기울기 역시 작아진다는 뜻이다. 실제로 이런 경우에는 학습이 잘 이뤄지지 않는다.

`Xavier 초깃값일 때`, 층이 깊을수록 치우침이 조금씩 커진다. 실제로 층이 깊어지면 활성화값들의 치우침이 커지고, `기울기 소실`문제를 일으킨다.

``He 초깃값일 때`, 모든 층에서 균일하게 분포하고있다.층이 깊어져도 분포가 균일하기 때문에 역전파 때에도 적절한 값이 나올 것이다.

### 6.2.4 MNIST 데이터셋으로 본 가중치 초깃값 비교

<img src="6장_학습 관련 기술들.assets/fig 6-15.png">

층별 뉴런 수가 100개인 5층 신경망, 활성화 함수는 ReLU

`std=0.01일 때는` 학습이 이루어지지 않았다. 순전파 때 값이 너무 작기(거의 0) 때문이다.

`Xavier, He 초깃값일`때 는 학습이 잘이루어졌고, He 초깃값이 진도가 더 빠르다.

### 6.3 배치 정규화

`배치 정규화`는 각 층의 활성화 값을 적당히 퍼지도록 `강제`하는 방법이다

`배치 정규화`가 주목받는 이유

- 학습을 빨리 진행할 수 있다.
- 초깃값에 크게 의존하지 않는다.
- 오버피티을 억제한다.

<img src="6장_학습 관련 기술들.assets/fig 6-16.png">

활성화 계층 전에 `배치 정규화 계층`을 삽입한다.

> 배치 정규화 수식, 평균 0 분산 1이 되도록 정규화 한다.

<img src="6장_학습 관련 기술들.assets/e 6.7.png">

<img src="6장_학습 관련 기술들.assets/e 6.8.png">

1. 미니배치`x1,x2,...,xm`에 대한 평균과 분산을 구한다.
2. 데이터를  평균 0, 분산 1 이 되게 정규화 한다.(`엡실론`은 0으로 나누는 사태를 예방한다.)
3. 정규화된 데이터에 고유한 확대와 이동 변환을 수행한다.(`감마`는 확대, `베타`는 이동을 담당한다.)

> 배치 정규화의 계산 그래프

<img src="6장_학습 관련 기술들.assets/fig 6-17.png">

### 6.3.2 배치 정규화 효과

> 배치 정규화한 것과 하지 않은 것의 학습 속도 차이 그래프

<img src="6장_학습 관련 기술들.assets/fig 6-18.png">

> 가중치 초깃값의 표준편차를 바꿔가면서 진행한 결과 비교

<img src="6장_학습 관련 기술들.assets/fig 6-19.png">

`실선`이 배치 정규화를 했을 때, `점선`이 하지 않았을 때 이다. 배치 정규화를 하지 않았을 때에는 학습이 거의 되지않았다.



## 6.4 바른 학습을 위해

기계학습에서는 `오버피팅`이 문제가 되는 일이 많다.

`오버피팅`은 훈련데이터에만 지나치게 적응되어 다른 데이터에는 제대로 대응하지 못하는 상태를 말한다.

### 6.4.1

> 오버피팅의 원인

1. 매개변수가 많고 표현력이 높은 모델
2. 훈련 데이터가 적음

> MNIST 데이터 60000개 중 300개만 사용, 7층 네트워크, 각 층의 뉴런은 100개, 활성화 함수는 ReLU의 학습 진행 그래프

<img src="6장_학습 관련 기술들.assets/fig 6-20.png">

`train` 데이터는 100 epochs 일 때 거의 정확도가 100%이지만 `test`데이터는 많은 차이를 보여준다.

이것은 모델이 `train`데이터에 너무 적응한 결과이다

### 6.4.2 가중치 감소

오버피팅 억제용으로 예전부터 `가중치 감소`방법을 이용해 왔다.

`가중치 감소`는 학습 과정에서 큰 가중치에 상응하는 큰 페널티를 부과해서 오버피팅을 억제하는 방법이다.

`오버피팅`은 가중치의 값이 커서 발생하는 경우가 많기 때문이다.

---

만약, 가중치의 제곱 노름`L2 노름`을 손실 함수에 더한다면 가중치가 커지는 것을 억제할 수 있다.

> 왜 손실함수가 커지면 가중치가 커지는 것을 억제하는건가?
>
> 손실함수가 커지면 학습 정도가 커지고, 특정데이터에 대한 적응도가 줄어든다.(?)

가중치를 `w`라 하면 L2노름에 따른 가중치 감소는 `(λw^2)/2 `이 되고, `(λw^2)/2 `를 손실 함수에 더한다. 

> `λ`는 정규화의 세기를 조절하는 하이퍼파라미터이다. `λ`가 커질수록 큰 가중치에 대한 페널티가 커진다.
>
> `(λw^2)/2 `의 1/2 는 `(λw^2)/2`의 미분 결과인 `λW`를 조정하는 상수이다.

가중치 감소는 모든 가중치 각각의 손실 함수에 `(λw^2)/2`를 더한다.\

> λ = 0.1로 가중치 감소를 적용한 결과

<img src="6장_학습 관련 기술들.assets/fig 6-21.png">

훈련데이터와 시험데이터의 정확도 차이가 아직 있지만`가중치 감소`를 적용하지 않은것에 비해서 차이가 줄었다.

### 6.4.3 드롭아웃

신경망 모델이 복잡해지면 `가중치 감소`만으로는 오버피팅을 억제하기가 어렵다.

그래서 `드롭아웃`이라는 기법을 이용한다.

`드롭아웃`은 뉴런을 임의로 삭제하면서 학습하는 방법이다. `훈련 때` 무작위로 은닉층의 뉴런을 무작위로 골라 삭제한다.

단, `시험 때`는 모든 뉴런에 신호를 전달하고, 뉴런의 출력에 훈련 때 삭제 안 한 비율을 곱하여 출력한다.

> 뉴런이 삭제됐을 때의 그림

<img src="6장_학습 관련 기술들.assets/fig 6-22.png">

> 드롭아웃 구현 코드

```python
import numpy as np


class Dropout:
    def __init__(self, dropout_ratio=0.5):
        self.dropout_ratio = dropout_ratio
        self.mask = None

    def forward(self, x, train_flag=True):
        # x : 입력
        # 훈련시에 드롭아웃을 한다.
        if train_flag:
            # mask : 입력과 똑같은 shape의 랜덤 값을 가진 행열
            # 임계치 보다 큰 값만 넘긴다. 작으면 삭제 = 드롭아웃
            self.mask = np.random.rand(*x.shape) > self.dropout_ratio
            return x * self.mask
        else:
            # 검증할 때에는 삭제 안한 비율을 곱해서 출력한다.
            return x * (1.0 - self.dropout_ratio)

    def backeard(self, dout):
        # 순전파 때 통과 시킨 신호만 통과
        return dout * self.mask
```

> 드롭아웃 차이

<img src="6장_학습 관련 기술들.assets/fig 6-23.png">

훈련 데이터와 시험 데이터에 대한 정확도 차이가 줄었고, 정확도가 100%에 도달하지 않는다.

## 6.5 적절한 하이퍼파라미터 값 찾기

각 층의 뉴런 수, 배치 크기, 매개변수 갱신시의 학습률과 가중치 감소 등이 `하이퍼 파라미터`에 속한다.

### 6.5.1 검증 데이터

데이터셋을 `훈련 데이터`와 `시험 데이터`로 분리해서 훈련 데이터로는 학습을 하고, 시험 데이터로는 범용 성능을 평가했다.

`하이퍼파라미터`를 다양하게 설정하고 검증하고, 성능을 평가할 때는 `시험 데이터`를 사용해서 안 된다는 것이다.

그 이유는 `하이퍼파라미터`값이 `시험 데이터`에 `오버피팅`되기 때문이다. 즉, 하이퍼파리미터가 시험 데이터에 적응되버린다는 뜻이다.

그래서 하이퍼파라미터를 조정할 때에는 `검증 데이터`를 만들어서 사용한다.

> `훈련(train) 데이터` : 매개변수 학습
>
> `검증(validation) 데이터` : 하이퍼파라미터 성능 평가
>
> `시험(test) 데이터` : 신경망의 범용 성능 평가

MNIST 데이터셋에서 검증 데이터를 얻는 가장 간단한 방법은 훈련 데이터 중 20% 정도를 검증 데이터로 먼저 분리하는 것이다.

> 코드 구현

```python
(x_train, t_train), (x_test, t_test) = load_mnist()

# 훈련 데이터 셔플
x_train, t_train = shuffle_dataset(x_train, t_train)

# 20%를 검증 데이터로 분할
validation_rate = 0.2
validation_num = int(x_train.shape[0] * validation_rate)

x_val = x_train[:validation_num]
t_val = t_train[:validation_num]
x_train = x_train[validation_num:]
t_train = t_train[validation_num:]
```

### 6.5.2 하이퍼파라미터 최적화

하이퍼파라미터를 최적화할 때의 핵심은 `최적 값`이 존재하는 범위를 조금씩 줄여간다는 것이다.

범위를 조금씩 줄일리면

1. 대략적인 범위를 설정하고
2. 범위 내에서 무작위로 하이퍼파라미터 값을 골라낸(샘플링) 후,
3. 그 값으로 정확도를 평가한다.

4. 이 작업을 반복하여 `최적 값`의 범위를 좁혀간다.

하이퍼파라미터의 범위는 `대략적으로` 지정하는 것이 효과적이다. 일반적으로 `로그 스케일(0.0001 ~ 1000 사이)`로 지정한다.

하이퍼파라미터 최적화 때는 오랜 시간이 걸리기 때문에

나쁠 듯한 값은 일찍 포기하고,

학습을 위한 에폭을 작게해서 1회 평가에 걸리는 시간을 단축하는 것이 효과적이다.

> 하이퍼파라미터 최적화 정리

1. 하이퍼파라미터 값의 범위를 `대략적으로` 설정
2. 설정된 범위에서 하이퍼파라미터 값을 무작위`샘플링`로 추출
3. 샘플링한 값을 사용하여 학습하고, 검증 데이터로 정확도 평가`단, 에포크는 작게 설정`
4. 2와 3을 특정 횟수(100회 등) 반복하여, 그 정확도의 결과를 보고 하이퍼파라미터 `범위를 좁힌다.`

> 위의 방법은 직관에 의존한다는 느낌이 있다. 더 엄밀하고 효율적으로 최적화를 수행하기 위해서는 `베이즈 최적화`를 사용하면 된다.

### 6.5.3 하이퍼파라미터 최적화 구현하기

>  `학습률`과 `가중치 감소의 세기`를 조절하는 계수를 탐색하는 코드()

```python
# np.random.uniform(low, high, size) : 최솟값이 low 최댓값이 high인 구간에서 size개의 난수 생성

# 하이퍼파라미터 검증 반복 횟수
optimization_trial = 100
# 그래프 생성 위한 결과 저장
results_val = {}
results_train = {}
for _ in range(optimization_trial):
    # 탐색한 하이퍼파라미터의 범위 지정===============
    weight_decay = 10 ** np.random.uniform(-8, -4)
    lr = 10 ** np.random.uniform(-6, -2)
    # ================================================
	
    # 학습률과 가중치 감소 조절 계수를 인자로 학습
    # 학습 결과(val_acc_list, train_acc_list) 반반환
    val_acc_list, train_acc_list = __train(lr, weight_decay)
    print("val acc:" + str(val_acc_list[-1]) + " | lr:" + str(lr) + ", weight decay:" + str(weight_decay))
    # {학습률 + 가중치 감소 조절 계수 : 정확도} 형태로 저장
    key = "lr:" + str(lr) + ", weight decay:" + str(weight_decay)
    results_val[key] = val_acc_list
    results_train[key] = train_acc_list

"""
weight_decay = np.random.uniform(10 ** -8, 10 ** -4)
lr = np.random.uniform(10 ** -6.0 , 10 ** -2)
가아닌이유??
"""
```

> 실선은 검증 데이터, 점선은 훈련 데이터에 대한 정확도 

<img src="../deep_learning_images/fig 6-24.png">

Best-5 까지는 학습이 순조롭게 진행되고 있는 것을 확인할 수 있다.